2023-07-18 15:23:15,488:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-18 15:23:15,488:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-18 15:23:15,488:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-18 15:23:15,488:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-18 15:26:33,276:INFO:PyCaret ClassificationExperiment
2023-07-18 15:26:33,302:INFO:Logging name: clf-default-name
2023-07-18 15:26:33,303:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-18 15:26:33,303:INFO:version 3.0.4
2023-07-18 15:26:33,303:INFO:Initializing setup()
2023-07-18 15:26:33,303:INFO:self.USI: c6f5
2023-07-18 15:26:33,303:INFO:self._variable_keys: {'target_param', 'html_param', 'X_train', 'memory', 'log_plots_param', 'logging_param', 'exp_id', 'n_jobs_param', 'y', 'data', 'X', 'is_multiclass', 'USI', '_ml_usecase', 'fix_imbalance', 'y_test', 'fold_groups_param', '_available_plots', 'seed', 'fold_generator', 'X_test', 'y_train', 'idx', 'gpu_n_jobs_param', 'pipeline', 'gpu_param', 'fold_shuffle_param', 'exp_name_log'}
2023-07-18 15:26:33,303:INFO:Checking environment
2023-07-18 15:26:33,304:INFO:python_version: 3.10.8
2023-07-18 15:26:33,304:INFO:python_build: ('main', 'Oct 12 2022 19:14:26')
2023-07-18 15:26:33,304:INFO:machine: x86_64
2023-07-18 15:26:33,304:INFO:platform: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 15:26:33,305:INFO:Memory: svmem(total=3928088576, available=819642368, percent=79.1, used=2230300672, free=355864576, active=1081794560, inactive=1830531072, buffers=53391360, cached=1288531968, shared=558784512, slab=324792320)
2023-07-18 15:26:33,306:INFO:Physical Core: 2
2023-07-18 15:26:33,306:INFO:Logical Core: 2
2023-07-18 15:26:33,306:INFO:Checking libraries
2023-07-18 15:26:33,307:INFO:System:
2023-07-18 15:26:33,307:INFO:    python: 3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]
2023-07-18 15:26:33,307:INFO:executable: /bin/python3.10
2023-07-18 15:26:33,316:INFO:   machine: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 15:26:33,316:INFO:PyCaret required dependencies:
2023-07-18 15:26:39,413:INFO:                 pip: 20.0.2
2023-07-18 15:26:39,413:INFO:          setuptools: 45.2.0
2023-07-18 15:26:39,414:INFO:             pycaret: 3.0.4
2023-07-18 15:26:39,414:INFO:             IPython: 7.34.0
2023-07-18 15:26:39,414:INFO:          ipywidgets: 7.7.5
2023-07-18 15:26:39,414:INFO:                tqdm: 4.65.0
2023-07-18 15:26:39,414:INFO:               numpy: 1.17.4
2023-07-18 15:26:39,414:INFO:              pandas: 1.5.3
2023-07-18 15:26:39,415:INFO:              jinja2: 3.1.2
2023-07-18 15:26:39,415:INFO:               scipy: 1.8.1
2023-07-18 15:26:39,415:INFO:              joblib: 1.2.0
2023-07-18 15:26:39,415:INFO:             sklearn: 1.2.2
2023-07-18 15:26:39,415:INFO:                pyod: 1.1.0
2023-07-18 15:26:39,415:INFO:            imblearn: 0.11.0
2023-07-18 15:26:39,415:INFO:   category_encoders: 2.6.1
2023-07-18 15:26:39,416:INFO:            lightgbm: 4.0.0
2023-07-18 15:26:39,416:INFO:               numba: 0.57.1
2023-07-18 15:26:39,416:INFO:            requests: 2.22.0
2023-07-18 15:26:39,416:INFO:          matplotlib: 3.1.2
2023-07-18 15:26:39,416:INFO:          scikitplot: 0.3.7
2023-07-18 15:26:39,416:INFO:         yellowbrick: 1.5
2023-07-18 15:26:39,416:INFO:              plotly: 5.14.0
2023-07-18 15:26:39,416:INFO:    plotly-resampler: Not installed
2023-07-18 15:26:39,416:INFO:             kaleido: 0.2.1
2023-07-18 15:26:39,417:INFO:           schemdraw: 0.15
2023-07-18 15:26:39,417:INFO:         statsmodels: 0.13.5
2023-07-18 15:26:39,417:INFO:              sktime: 0.20.1
2023-07-18 15:26:39,417:INFO:               tbats: 1.1.3
2023-07-18 15:26:39,417:INFO:            pmdarima: 2.0.3
2023-07-18 15:26:39,417:INFO:              psutil: 5.9.4
2023-07-18 15:26:39,417:INFO:          markupsafe: 1.1.0
2023-07-18 15:26:39,417:INFO:             pickle5: Not installed
2023-07-18 15:26:39,418:INFO:         cloudpickle: 2.2.1
2023-07-18 15:26:39,418:INFO:         deprecation: 2.1.0
2023-07-18 15:26:39,418:INFO:              xxhash: 3.2.0
2023-07-18 15:26:39,418:INFO:           wurlitzer: 3.0.3
2023-07-18 15:26:39,418:INFO:PyCaret optional dependencies:
2023-07-18 15:26:48,279:INFO:                shap: 0.42.0
2023-07-18 15:26:48,279:INFO:           interpret: 0.4.2
2023-07-18 15:26:48,280:INFO:                umap: 0.5.3
2023-07-18 15:26:48,280:INFO:    pandas_profiling: 4.3.1
2023-07-18 15:26:48,280:INFO:  explainerdashboard: 0.4.2.2
2023-07-18 15:26:48,280:INFO:             autoviz: 0.1.730
2023-07-18 15:26:48,280:INFO:           fairlearn: 0.7.0
2023-07-18 15:26:48,280:INFO:          deepchecks: 0.17.3
2023-07-18 15:26:48,280:INFO:             xgboost: 1.7.6
2023-07-18 15:26:48,281:INFO:            catboost: 1.2
2023-07-18 15:26:48,281:INFO:              kmodes: 0.12.2
2023-07-18 15:26:48,281:INFO:             mlxtend: 0.22.0
2023-07-18 15:26:48,281:INFO:       statsforecast: 1.5.0
2023-07-18 15:26:48,281:INFO:        tune_sklearn: 0.4.6
2023-07-18 15:26:48,281:INFO:                 ray: 2.5.1
2023-07-18 15:26:48,281:INFO:            hyperopt: 0.2.7
2023-07-18 15:26:48,281:INFO:              optuna: 3.2.0
2023-07-18 15:26:48,282:INFO:               skopt: 0.9.0
2023-07-18 15:26:48,282:INFO:              mlflow: 1.30.1
2023-07-18 15:26:48,282:INFO:              gradio: 3.37.0
2023-07-18 15:26:48,282:INFO:             fastapi: 0.100.0
2023-07-18 15:26:48,282:INFO:             uvicorn: 0.23.1
2023-07-18 15:26:48,282:INFO:              m2cgen: 0.10.0
2023-07-18 15:26:48,282:INFO:           evidently: 0.2.8
2023-07-18 15:26:48,282:INFO:               fugue: 0.8.5
2023-07-18 15:26:48,283:INFO:           streamlit: 1.22.0
2023-07-18 15:26:48,283:INFO:             prophet: Not installed
2023-07-18 15:26:48,283:INFO:None
2023-07-18 15:26:48,283:INFO:Set up data.
2023-07-18 15:37:07,822:INFO:PyCaret ClassificationExperiment
2023-07-18 15:37:07,843:INFO:Logging name: clf-default-name
2023-07-18 15:37:07,844:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-18 15:37:07,844:INFO:version 3.0.4
2023-07-18 15:37:07,844:INFO:Initializing setup()
2023-07-18 15:37:07,844:INFO:self.USI: b425
2023-07-18 15:37:07,844:INFO:self._variable_keys: {'target_param', 'html_param', 'X_train', 'memory', 'log_plots_param', 'logging_param', 'exp_id', 'n_jobs_param', 'y', 'data', 'X', 'is_multiclass', 'USI', '_ml_usecase', 'fix_imbalance', 'y_test', 'fold_groups_param', '_available_plots', 'seed', 'fold_generator', 'X_test', 'y_train', 'idx', 'gpu_n_jobs_param', 'pipeline', 'gpu_param', 'fold_shuffle_param', 'exp_name_log'}
2023-07-18 15:37:07,844:INFO:Checking environment
2023-07-18 15:37:07,845:INFO:python_version: 3.10.8
2023-07-18 15:37:07,845:INFO:python_build: ('main', 'Oct 12 2022 19:14:26')
2023-07-18 15:37:07,845:INFO:machine: x86_64
2023-07-18 15:37:07,845:INFO:platform: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 15:37:07,851:INFO:Memory: svmem(total=3928088576, available=701001728, percent=82.2, used=2354618368, free=203509760, active=1447690240, inactive=1521270784, buffers=23801856, cached=1346158592, shared=578834432, slab=315691008)
2023-07-18 15:37:07,852:INFO:Physical Core: 2
2023-07-18 15:37:07,853:INFO:Logical Core: 2
2023-07-18 15:37:07,858:INFO:Checking libraries
2023-07-18 15:37:07,859:INFO:System:
2023-07-18 15:37:07,859:INFO:    python: 3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]
2023-07-18 15:37:07,860:INFO:executable: /bin/python3.10
2023-07-18 15:37:07,860:INFO:   machine: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 15:37:07,860:INFO:PyCaret required dependencies:
2023-07-18 15:37:07,861:INFO:                 pip: 20.0.2
2023-07-18 15:37:07,861:INFO:          setuptools: 45.2.0
2023-07-18 15:37:07,862:INFO:             pycaret: 3.0.4
2023-07-18 15:37:07,862:INFO:             IPython: 7.34.0
2023-07-18 15:37:07,867:INFO:          ipywidgets: 7.7.5
2023-07-18 15:37:07,871:INFO:                tqdm: 4.65.0
2023-07-18 15:37:07,872:INFO:               numpy: 1.17.4
2023-07-18 15:37:07,882:INFO:              pandas: 1.5.3
2023-07-18 15:37:07,883:INFO:              jinja2: 3.1.2
2023-07-18 15:37:07,883:INFO:               scipy: 1.8.1
2023-07-18 15:37:07,892:INFO:              joblib: 1.2.0
2023-07-18 15:37:07,898:INFO:             sklearn: 1.2.2
2023-07-18 15:37:07,899:INFO:                pyod: 1.1.0
2023-07-18 15:37:07,902:INFO:            imblearn: 0.11.0
2023-07-18 15:37:07,907:INFO:   category_encoders: 2.6.1
2023-07-18 15:37:07,907:INFO:            lightgbm: 4.0.0
2023-07-18 15:37:07,908:INFO:               numba: 0.57.1
2023-07-18 15:37:07,908:INFO:            requests: 2.22.0
2023-07-18 15:37:07,908:INFO:          matplotlib: 3.1.2
2023-07-18 15:37:07,911:INFO:          scikitplot: 0.3.7
2023-07-18 15:37:07,923:INFO:         yellowbrick: 1.5
2023-07-18 15:37:07,924:INFO:              plotly: 5.14.0
2023-07-18 15:37:07,927:INFO:    plotly-resampler: Not installed
2023-07-18 15:37:07,932:INFO:             kaleido: 0.2.1
2023-07-18 15:37:07,932:INFO:           schemdraw: 0.15
2023-07-18 15:37:07,935:INFO:         statsmodels: 0.13.5
2023-07-18 15:37:07,939:INFO:              sktime: 0.20.1
2023-07-18 15:37:07,945:INFO:               tbats: 1.1.3
2023-07-18 15:37:07,945:INFO:            pmdarima: 2.0.3
2023-07-18 15:37:07,946:INFO:              psutil: 5.9.4
2023-07-18 15:37:07,946:INFO:          markupsafe: 1.1.0
2023-07-18 15:37:07,946:INFO:             pickle5: Not installed
2023-07-18 15:37:07,947:INFO:         cloudpickle: 2.2.1
2023-07-18 15:37:07,947:INFO:         deprecation: 2.1.0
2023-07-18 15:37:07,953:INFO:              xxhash: 3.2.0
2023-07-18 15:37:07,955:INFO:           wurlitzer: 3.0.3
2023-07-18 15:37:07,961:INFO:PyCaret optional dependencies:
2023-07-18 15:37:07,962:INFO:                shap: 0.42.0
2023-07-18 15:37:07,962:INFO:           interpret: 0.4.2
2023-07-18 15:37:07,969:INFO:                umap: 0.5.3
2023-07-18 15:37:07,980:INFO:    pandas_profiling: 4.3.1
2023-07-18 15:37:07,980:INFO:  explainerdashboard: 0.4.2.2
2023-07-18 15:37:07,982:INFO:             autoviz: 0.1.730
2023-07-18 15:37:07,982:INFO:           fairlearn: 0.7.0
2023-07-18 15:37:07,987:INFO:          deepchecks: 0.17.3
2023-07-18 15:37:07,995:INFO:             xgboost: 1.7.6
2023-07-18 15:37:08,002:INFO:            catboost: 1.2
2023-07-18 15:37:08,011:INFO:              kmodes: 0.12.2
2023-07-18 15:37:08,011:INFO:             mlxtend: 0.22.0
2023-07-18 15:37:08,012:INFO:       statsforecast: 1.5.0
2023-07-18 15:37:08,012:INFO:        tune_sklearn: 0.4.6
2023-07-18 15:37:08,012:INFO:                 ray: 2.5.1
2023-07-18 15:37:08,013:INFO:            hyperopt: 0.2.7
2023-07-18 15:37:08,013:INFO:              optuna: 3.2.0
2023-07-18 15:37:08,013:INFO:               skopt: 0.9.0
2023-07-18 15:37:08,013:INFO:              mlflow: 1.30.1
2023-07-18 15:37:08,015:INFO:              gradio: 3.37.0
2023-07-18 15:37:08,017:INFO:             fastapi: 0.100.0
2023-07-18 15:37:08,017:INFO:             uvicorn: 0.23.1
2023-07-18 15:37:08,018:INFO:              m2cgen: 0.10.0
2023-07-18 15:37:08,018:INFO:           evidently: 0.2.8
2023-07-18 15:37:08,024:INFO:               fugue: 0.8.5
2023-07-18 15:37:08,027:INFO:           streamlit: 1.22.0
2023-07-18 15:37:08,033:INFO:             prophet: Not installed
2023-07-18 15:37:08,044:INFO:None
2023-07-18 15:37:08,044:INFO:Set up data.
2023-07-18 15:39:08,714:INFO:PyCaret ClassificationExperiment
2023-07-18 15:39:08,723:INFO:Logging name: clf-default-name
2023-07-18 15:39:08,724:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-18 15:39:08,726:INFO:version 3.0.4
2023-07-18 15:39:08,727:INFO:Initializing setup()
2023-07-18 15:39:08,729:INFO:self.USI: e513
2023-07-18 15:39:08,731:INFO:self._variable_keys: {'target_param', 'html_param', 'X_train', 'memory', 'log_plots_param', 'logging_param', 'exp_id', 'n_jobs_param', 'y', 'data', 'X', 'is_multiclass', 'USI', '_ml_usecase', 'fix_imbalance', 'y_test', 'fold_groups_param', '_available_plots', 'seed', 'fold_generator', 'X_test', 'y_train', 'idx', 'gpu_n_jobs_param', 'pipeline', 'gpu_param', 'fold_shuffle_param', 'exp_name_log'}
2023-07-18 15:39:08,732:INFO:Checking environment
2023-07-18 15:39:08,734:INFO:python_version: 3.10.8
2023-07-18 15:39:08,735:INFO:python_build: ('main', 'Oct 12 2022 19:14:26')
2023-07-18 15:39:08,743:INFO:machine: x86_64
2023-07-18 15:39:08,745:INFO:platform: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 15:39:08,750:INFO:Memory: svmem(total=3928088576, available=663609344, percent=83.1, used=2407317504, free=148307968, active=1450381312, inactive=1575534592, buffers=30789632, cached=1341673472, shared=563593216, slab=315715584)
2023-07-18 15:39:08,754:INFO:Physical Core: 2
2023-07-18 15:39:08,775:INFO:Logical Core: 2
2023-07-18 15:39:08,776:INFO:Checking libraries
2023-07-18 15:39:08,783:INFO:System:
2023-07-18 15:39:08,784:INFO:    python: 3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]
2023-07-18 15:39:08,784:INFO:executable: /bin/python3.10
2023-07-18 15:39:08,784:INFO:   machine: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 15:39:08,784:INFO:PyCaret required dependencies:
2023-07-18 15:39:08,789:INFO:                 pip: 20.0.2
2023-07-18 15:39:08,793:INFO:          setuptools: 45.2.0
2023-07-18 15:39:08,794:INFO:             pycaret: 3.0.4
2023-07-18 15:39:08,794:INFO:             IPython: 7.34.0
2023-07-18 15:39:08,795:INFO:          ipywidgets: 7.7.5
2023-07-18 15:39:08,799:INFO:                tqdm: 4.65.0
2023-07-18 15:39:08,804:INFO:               numpy: 1.17.4
2023-07-18 15:39:08,804:INFO:              pandas: 1.5.3
2023-07-18 15:39:08,805:INFO:              jinja2: 3.1.2
2023-07-18 15:39:08,805:INFO:               scipy: 1.8.1
2023-07-18 15:39:08,805:INFO:              joblib: 1.2.0
2023-07-18 15:39:08,805:INFO:             sklearn: 1.2.2
2023-07-18 15:39:08,811:INFO:                pyod: 1.1.0
2023-07-18 15:39:08,812:INFO:            imblearn: 0.11.0
2023-07-18 15:39:08,815:INFO:   category_encoders: 2.6.1
2023-07-18 15:39:08,815:INFO:            lightgbm: 4.0.0
2023-07-18 15:39:08,826:INFO:               numba: 0.57.1
2023-07-18 15:39:08,827:INFO:            requests: 2.22.0
2023-07-18 15:39:08,827:INFO:          matplotlib: 3.1.2
2023-07-18 15:39:08,827:INFO:          scikitplot: 0.3.7
2023-07-18 15:39:08,828:INFO:         yellowbrick: 1.5
2023-07-18 15:39:08,828:INFO:              plotly: 5.14.0
2023-07-18 15:39:08,841:INFO:    plotly-resampler: Not installed
2023-07-18 15:39:08,842:INFO:             kaleido: 0.2.1
2023-07-18 15:39:08,842:INFO:           schemdraw: 0.15
2023-07-18 15:39:08,842:INFO:         statsmodels: 0.13.5
2023-07-18 15:39:08,843:INFO:              sktime: 0.20.1
2023-07-18 15:39:08,847:INFO:               tbats: 1.1.3
2023-07-18 15:39:08,850:INFO:            pmdarima: 2.0.3
2023-07-18 15:39:08,852:INFO:              psutil: 5.9.4
2023-07-18 15:39:08,854:INFO:          markupsafe: 1.1.0
2023-07-18 15:39:08,859:INFO:             pickle5: Not installed
2023-07-18 15:39:08,860:INFO:         cloudpickle: 2.2.1
2023-07-18 15:39:08,860:INFO:         deprecation: 2.1.0
2023-07-18 15:39:08,863:INFO:              xxhash: 3.2.0
2023-07-18 15:39:08,893:INFO:           wurlitzer: 3.0.3
2023-07-18 15:39:08,893:INFO:PyCaret optional dependencies:
2023-07-18 15:39:08,894:INFO:                shap: 0.42.0
2023-07-18 15:39:08,894:INFO:           interpret: 0.4.2
2023-07-18 15:39:08,894:INFO:                umap: 0.5.3
2023-07-18 15:39:08,895:INFO:    pandas_profiling: 4.3.1
2023-07-18 15:39:08,897:INFO:  explainerdashboard: 0.4.2.2
2023-07-18 15:39:08,897:INFO:             autoviz: 0.1.730
2023-07-18 15:39:08,897:INFO:           fairlearn: 0.7.0
2023-07-18 15:39:08,898:INFO:          deepchecks: 0.17.3
2023-07-18 15:39:08,898:INFO:             xgboost: 1.7.6
2023-07-18 15:39:08,898:INFO:            catboost: 1.2
2023-07-18 15:39:08,899:INFO:              kmodes: 0.12.2
2023-07-18 15:39:08,899:INFO:             mlxtend: 0.22.0
2023-07-18 15:39:08,899:INFO:       statsforecast: 1.5.0
2023-07-18 15:39:08,900:INFO:        tune_sklearn: 0.4.6
2023-07-18 15:39:08,908:INFO:                 ray: 2.5.1
2023-07-18 15:39:08,911:INFO:            hyperopt: 0.2.7
2023-07-18 15:39:08,924:INFO:              optuna: 3.2.0
2023-07-18 15:39:08,925:INFO:               skopt: 0.9.0
2023-07-18 15:39:08,925:INFO:              mlflow: 1.30.1
2023-07-18 15:39:08,929:INFO:              gradio: 3.37.0
2023-07-18 15:39:08,930:INFO:             fastapi: 0.100.0
2023-07-18 15:39:08,930:INFO:             uvicorn: 0.23.1
2023-07-18 15:39:08,931:INFO:              m2cgen: 0.10.0
2023-07-18 15:39:08,939:INFO:           evidently: 0.2.8
2023-07-18 15:39:08,940:INFO:               fugue: 0.8.5
2023-07-18 15:39:08,940:INFO:           streamlit: 1.22.0
2023-07-18 15:39:08,940:INFO:             prophet: Not installed
2023-07-18 15:39:08,940:INFO:None
2023-07-18 15:39:08,946:INFO:Set up data.
2023-07-18 15:49:17,791:INFO:PyCaret ClassificationExperiment
2023-07-18 15:49:17,791:INFO:Logging name: clf-default-name
2023-07-18 15:49:17,792:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-18 15:49:17,792:INFO:version 3.0.4
2023-07-18 15:49:17,792:INFO:Initializing setup()
2023-07-18 15:49:17,792:INFO:self.USI: ccb9
2023-07-18 15:49:17,792:INFO:self._variable_keys: {'target_param', 'html_param', 'X_train', 'memory', 'log_plots_param', 'logging_param', 'exp_id', 'n_jobs_param', 'y', 'data', 'X', 'is_multiclass', 'USI', '_ml_usecase', 'fix_imbalance', 'y_test', 'fold_groups_param', '_available_plots', 'seed', 'fold_generator', 'X_test', 'y_train', 'idx', 'gpu_n_jobs_param', 'pipeline', 'gpu_param', 'fold_shuffle_param', 'exp_name_log'}
2023-07-18 15:49:17,792:INFO:Checking environment
2023-07-18 15:49:17,792:INFO:python_version: 3.10.8
2023-07-18 15:49:17,792:INFO:python_build: ('main', 'Oct 12 2022 19:14:26')
2023-07-18 15:49:17,793:INFO:machine: x86_64
2023-07-18 15:49:17,793:INFO:platform: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 15:49:17,793:INFO:Memory: svmem(total=3928088576, available=841093120, percent=78.6, used=2259828736, free=332529664, active=1457676288, inactive=1438240768, buffers=29368320, cached=1306361856, shared=516669440, slab=316063744)
2023-07-18 15:49:17,794:INFO:Physical Core: 2
2023-07-18 15:49:17,794:INFO:Logical Core: 2
2023-07-18 15:49:17,794:INFO:Checking libraries
2023-07-18 15:49:17,794:INFO:System:
2023-07-18 15:49:17,794:INFO:    python: 3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]
2023-07-18 15:49:17,795:INFO:executable: /bin/python3.10
2023-07-18 15:49:17,795:INFO:   machine: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 15:49:17,795:INFO:PyCaret required dependencies:
2023-07-18 15:49:17,795:INFO:                 pip: 20.0.2
2023-07-18 15:49:17,795:INFO:          setuptools: 45.2.0
2023-07-18 15:49:17,795:INFO:             pycaret: 3.0.4
2023-07-18 15:49:17,795:INFO:             IPython: 7.34.0
2023-07-18 15:49:17,796:INFO:          ipywidgets: 7.7.5
2023-07-18 15:49:17,796:INFO:                tqdm: 4.65.0
2023-07-18 15:49:17,796:INFO:               numpy: 1.17.4
2023-07-18 15:49:17,796:INFO:              pandas: 1.5.3
2023-07-18 15:49:17,796:INFO:              jinja2: 3.1.2
2023-07-18 15:49:17,796:INFO:               scipy: 1.8.1
2023-07-18 15:49:17,796:INFO:              joblib: 1.2.0
2023-07-18 15:49:17,796:INFO:             sklearn: 1.2.2
2023-07-18 15:49:17,797:INFO:                pyod: 1.1.0
2023-07-18 15:49:17,797:INFO:            imblearn: 0.11.0
2023-07-18 15:49:17,797:INFO:   category_encoders: 2.6.1
2023-07-18 15:49:17,797:INFO:            lightgbm: 4.0.0
2023-07-18 15:49:17,797:INFO:               numba: 0.57.1
2023-07-18 15:49:17,797:INFO:            requests: 2.22.0
2023-07-18 15:49:17,797:INFO:          matplotlib: 3.1.2
2023-07-18 15:49:17,797:INFO:          scikitplot: 0.3.7
2023-07-18 15:49:17,797:INFO:         yellowbrick: 1.5
2023-07-18 15:49:17,798:INFO:              plotly: 5.14.0
2023-07-18 15:49:17,798:INFO:    plotly-resampler: Not installed
2023-07-18 15:49:17,798:INFO:             kaleido: 0.2.1
2023-07-18 15:49:17,798:INFO:           schemdraw: 0.15
2023-07-18 15:49:17,798:INFO:         statsmodels: 0.13.5
2023-07-18 15:49:17,798:INFO:              sktime: 0.20.1
2023-07-18 15:49:17,798:INFO:               tbats: 1.1.3
2023-07-18 15:49:17,798:INFO:            pmdarima: 2.0.3
2023-07-18 15:49:17,799:INFO:              psutil: 5.9.4
2023-07-18 15:49:17,799:INFO:          markupsafe: 1.1.0
2023-07-18 15:49:17,799:INFO:             pickle5: Not installed
2023-07-18 15:49:17,808:INFO:         cloudpickle: 2.2.1
2023-07-18 15:49:17,808:INFO:         deprecation: 2.1.0
2023-07-18 15:49:17,808:INFO:              xxhash: 3.2.0
2023-07-18 15:49:17,809:INFO:           wurlitzer: 3.0.3
2023-07-18 15:49:17,809:INFO:PyCaret optional dependencies:
2023-07-18 15:49:17,810:INFO:                shap: 0.42.0
2023-07-18 15:49:17,810:INFO:           interpret: 0.4.2
2023-07-18 15:49:17,810:INFO:                umap: 0.5.3
2023-07-18 15:49:17,813:INFO:    pandas_profiling: 4.3.1
2023-07-18 15:49:17,814:INFO:  explainerdashboard: 0.4.2.2
2023-07-18 15:49:17,814:INFO:             autoviz: 0.1.730
2023-07-18 15:49:17,831:INFO:           fairlearn: 0.7.0
2023-07-18 15:49:17,831:INFO:          deepchecks: 0.17.3
2023-07-18 15:49:17,832:INFO:             xgboost: 1.7.6
2023-07-18 15:49:17,832:INFO:            catboost: 1.2
2023-07-18 15:49:17,835:INFO:              kmodes: 0.12.2
2023-07-18 15:49:17,838:INFO:             mlxtend: 0.22.0
2023-07-18 15:49:17,847:INFO:       statsforecast: 1.5.0
2023-07-18 15:49:17,847:INFO:        tune_sklearn: 0.4.6
2023-07-18 15:49:17,851:INFO:                 ray: 2.5.1
2023-07-18 15:49:17,851:INFO:            hyperopt: 0.2.7
2023-07-18 15:49:17,852:INFO:              optuna: 3.2.0
2023-07-18 15:49:17,859:INFO:               skopt: 0.9.0
2023-07-18 15:49:17,860:INFO:              mlflow: 1.30.1
2023-07-18 15:49:17,860:INFO:              gradio: 3.37.0
2023-07-18 15:49:17,861:INFO:             fastapi: 0.100.0
2023-07-18 15:49:17,861:INFO:             uvicorn: 0.23.1
2023-07-18 15:49:17,861:INFO:              m2cgen: 0.10.0
2023-07-18 15:49:17,862:INFO:           evidently: 0.2.8
2023-07-18 15:49:17,862:INFO:               fugue: 0.8.5
2023-07-18 15:49:17,862:INFO:           streamlit: 1.22.0
2023-07-18 15:49:17,863:INFO:             prophet: Not installed
2023-07-18 15:49:17,868:INFO:None
2023-07-18 15:49:17,884:INFO:Set up data.
2023-07-18 16:07:07,526:INFO:PyCaret ClassificationExperiment
2023-07-18 16:07:07,687:INFO:Logging name: clf-default-name
2023-07-18 16:07:07,687:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-18 16:07:07,687:INFO:version 3.0.4
2023-07-18 16:07:07,687:INFO:Initializing setup()
2023-07-18 16:07:07,688:INFO:self.USI: 8218
2023-07-18 16:07:07,688:INFO:self._variable_keys: {'target_param', 'html_param', 'X_train', 'memory', 'log_plots_param', 'logging_param', 'exp_id', 'n_jobs_param', 'y', 'data', 'X', 'is_multiclass', 'USI', '_ml_usecase', 'fix_imbalance', 'y_test', 'fold_groups_param', '_available_plots', 'seed', 'fold_generator', 'X_test', 'y_train', 'idx', 'gpu_n_jobs_param', 'pipeline', 'gpu_param', 'fold_shuffle_param', 'exp_name_log'}
2023-07-18 16:07:07,745:INFO:Checking environment
2023-07-18 16:07:07,746:INFO:python_version: 3.10.8
2023-07-18 16:07:07,746:INFO:python_build: ('main', 'Oct 12 2022 19:14:26')
2023-07-18 16:07:07,746:INFO:machine: x86_64
2023-07-18 16:07:07,746:INFO:platform: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 16:07:07,746:INFO:Memory: svmem(total=3928088576, available=1202327552, percent=69.4, used=1985069056, free=408133632, active=1592643584, inactive=1276481536, buffers=37277696, cached=1497608192, shared=447016960, slab=315559936)
2023-07-18 16:07:07,747:INFO:Physical Core: 2
2023-07-18 16:07:07,747:INFO:Logical Core: 2
2023-07-18 16:07:07,748:INFO:Checking libraries
2023-07-18 16:07:07,748:INFO:System:
2023-07-18 16:07:07,748:INFO:    python: 3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]
2023-07-18 16:07:07,748:INFO:executable: /bin/python3.10
2023-07-18 16:07:07,748:INFO:   machine: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 16:07:07,748:INFO:PyCaret required dependencies:
2023-07-18 16:07:07,761:INFO:                 pip: 20.0.2
2023-07-18 16:07:07,761:INFO:          setuptools: 45.2.0
2023-07-18 16:07:07,761:INFO:             pycaret: 3.0.4
2023-07-18 16:07:07,761:INFO:             IPython: 7.34.0
2023-07-18 16:07:07,762:INFO:          ipywidgets: 7.7.5
2023-07-18 16:07:07,762:INFO:                tqdm: 4.65.0
2023-07-18 16:07:07,762:INFO:               numpy: 1.17.4
2023-07-18 16:07:07,762:INFO:              pandas: 1.5.3
2023-07-18 16:07:07,762:INFO:              jinja2: 3.1.2
2023-07-18 16:07:07,762:INFO:               scipy: 1.8.1
2023-07-18 16:07:07,762:INFO:              joblib: 1.2.0
2023-07-18 16:07:07,762:INFO:             sklearn: 1.2.2
2023-07-18 16:07:07,763:INFO:                pyod: 1.1.0
2023-07-18 16:07:07,763:INFO:            imblearn: 0.11.0
2023-07-18 16:07:07,763:INFO:   category_encoders: 2.6.1
2023-07-18 16:07:07,764:INFO:            lightgbm: 4.0.0
2023-07-18 16:07:07,764:INFO:               numba: 0.57.1
2023-07-18 16:07:07,765:INFO:            requests: 2.22.0
2023-07-18 16:07:07,765:INFO:          matplotlib: 3.1.2
2023-07-18 16:07:07,767:INFO:          scikitplot: 0.3.7
2023-07-18 16:07:07,767:INFO:         yellowbrick: 1.5
2023-07-18 16:07:07,767:INFO:              plotly: 5.14.0
2023-07-18 16:07:07,767:INFO:    plotly-resampler: Not installed
2023-07-18 16:07:07,767:INFO:             kaleido: 0.2.1
2023-07-18 16:07:07,768:INFO:           schemdraw: 0.15
2023-07-18 16:07:07,768:INFO:         statsmodels: 0.13.5
2023-07-18 16:07:07,768:INFO:              sktime: 0.20.1
2023-07-18 16:07:07,768:INFO:               tbats: 1.1.3
2023-07-18 16:07:07,768:INFO:            pmdarima: 2.0.3
2023-07-18 16:07:07,768:INFO:              psutil: 5.9.4
2023-07-18 16:07:07,768:INFO:          markupsafe: 1.1.0
2023-07-18 16:07:07,768:INFO:             pickle5: Not installed
2023-07-18 16:07:07,768:INFO:         cloudpickle: 2.2.1
2023-07-18 16:07:07,769:INFO:         deprecation: 2.1.0
2023-07-18 16:07:07,769:INFO:              xxhash: 3.2.0
2023-07-18 16:07:07,769:INFO:           wurlitzer: 3.0.3
2023-07-18 16:07:07,771:INFO:PyCaret optional dependencies:
2023-07-18 16:07:07,784:INFO:                shap: 0.42.0
2023-07-18 16:07:07,786:INFO:           interpret: 0.4.2
2023-07-18 16:07:07,786:INFO:                umap: 0.5.3
2023-07-18 16:07:07,786:INFO:    pandas_profiling: 4.3.1
2023-07-18 16:07:07,786:INFO:  explainerdashboard: 0.4.2.2
2023-07-18 16:07:07,786:INFO:             autoviz: 0.1.730
2023-07-18 16:07:07,787:INFO:           fairlearn: 0.7.0
2023-07-18 16:07:07,787:INFO:          deepchecks: 0.17.3
2023-07-18 16:07:07,787:INFO:             xgboost: 1.7.6
2023-07-18 16:07:07,787:INFO:            catboost: 1.2
2023-07-18 16:07:07,787:INFO:              kmodes: 0.12.2
2023-07-18 16:07:07,787:INFO:             mlxtend: 0.22.0
2023-07-18 16:07:07,787:INFO:       statsforecast: 1.5.0
2023-07-18 16:07:07,787:INFO:        tune_sklearn: 0.4.6
2023-07-18 16:07:07,788:INFO:                 ray: 2.5.1
2023-07-18 16:07:07,788:INFO:            hyperopt: 0.2.7
2023-07-18 16:07:07,788:INFO:              optuna: 3.2.0
2023-07-18 16:07:07,788:INFO:               skopt: 0.9.0
2023-07-18 16:07:07,788:INFO:              mlflow: 1.30.1
2023-07-18 16:07:07,788:INFO:              gradio: 3.37.0
2023-07-18 16:07:07,790:INFO:             fastapi: 0.100.0
2023-07-18 16:07:07,790:INFO:             uvicorn: 0.23.1
2023-07-18 16:07:07,790:INFO:              m2cgen: 0.10.0
2023-07-18 16:07:07,790:INFO:           evidently: 0.2.8
2023-07-18 16:07:07,790:INFO:               fugue: 0.8.5
2023-07-18 16:07:07,791:INFO:           streamlit: 1.22.0
2023-07-18 16:07:07,791:INFO:             prophet: Not installed
2023-07-18 16:07:07,791:INFO:None
2023-07-18 16:07:07,791:INFO:Set up data.
2023-07-18 16:07:12,428:INFO:Set up train/test split.
2023-07-18 16:07:14,982:INFO:Set up index.
2023-07-18 16:07:15,042:INFO:Set up folding strategy.
2023-07-18 16:07:15,187:INFO:Assigning column types.
2023-07-18 16:07:15,769:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-18 16:07:16,942:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-18 16:07:19,993:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-18 16:07:25,204:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:07:38,722:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:07:42,894:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-18 16:07:42,903:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-18 16:07:43,131:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:07:43,147:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:07:43,200:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-18 16:07:43,606:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-18 16:07:43,868:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:07:43,885:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:07:44,206:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-18 16:07:44,396:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:07:44,417:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:07:44,420:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-18 16:07:44,991:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:07:45,028:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:07:45,695:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:07:45,737:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:07:46,202:INFO:Preparing preprocessing pipeline...
2023-07-18 16:07:46,318:INFO:Set up label encoding.
2023-07-18 16:07:46,402:INFO:Set up iterative imputation.
2023-07-18 16:07:46,586:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-18 16:07:46,662:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-18 16:07:46,722:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-18 16:07:48,003:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-18 16:07:48,717:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:07:48,805:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:07:49,621:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:07:49,662:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:07:50,185:INFO:Set up encoding of ordinal features.
2023-07-18 16:07:50,396:INFO:Set up encoding of categorical features.
2023-07-18 16:07:57,021:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 2.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:07:59,758:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:08:01,982:INFO:Finished creating preprocessing pipeline.
2023-07-18 16:08:02,738:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsampl...
                                                                    'interferencia_tratamento_eficaz',
                                                                    'interferencia_sem_tratamento_eficaz',
                                                                    'observacoes_discussao_mental',
                                                                    'disposto_apresentar_saude_entrevista',
                                                                    'traria_saude_mental_entrevista',
                                                                    'apoio_setor_tecnologia_mental',
                                                                    'genero',
                                                                    'ano'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2023-07-18 16:08:02,739:INFO:Creating final display dataframe.
2023-07-18 16:08:06,627:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:271: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_full_transform(

2023-07-18 16:08:10,936:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:271: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_full_transform(

2023-07-18 16:08:11,863:INFO:Setup _display_container:                         Description                           Value
0                        Session id                              42
1                            Target  tratamento_profissional_mental
2                       Target type                          Binary
3                    Target mapping                   No: 0, Yes: 1
4               Original data shape                       (131, 36)
5            Transformed data shape                       (131, 87)
6       Transformed train set shape                        (91, 87)
7        Transformed test set shape                        (40, 87)
8                  Ordinal features                              10
9                  Numeric features                               5
10             Categorical features                              30
11                       Preprocess                            True
12                  Imputation type                       iterative
13  Iterative imputation iterations                               5
14        Numeric iterative imputer                        lightgbm
15    Categorical iterative imputer                        lightgbm
16         Maximum one-hot encoding                              25
17                  Encoding method                            None
18                   Fold Generator                 StratifiedKFold
19                      Fold Number                              10
20                         CPU Jobs                              -1
21                          Use GPU                           False
22                   Log Experiment                           False
23                  Experiment Name                clf-default-name
24                              USI                            8218
2023-07-18 16:08:14,762:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:08:14,791:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:08:15,366:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 16:08:15,388:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 16:08:15,635:INFO:setup() successfully completed in 68.81s...............
2023-07-18 16:09:12,134:INFO:Initializing compare_models()
2023-07-18 16:09:12,134:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-18 16:09:12,134:INFO:Checking exceptions
2023-07-18 16:09:12,533:INFO:Preparing display monitor
2023-07-18 16:09:15,412:INFO:Initializing Logistic Regression
2023-07-18 16:09:15,413:INFO:Total runtime is 0.00027068853378295896 minutes
2023-07-18 16:09:15,540:INFO:SubProcess create_model() called ==================================
2023-07-18 16:09:15,617:INFO:Initializing create_model()
2023-07-18 16:09:15,618:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f8d9949a1d0>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 16:09:15,618:INFO:Checking exceptions
2023-07-18 16:09:15,625:INFO:Importing libraries
2023-07-18 16:09:15,625:INFO:Copying training dataset
2023-07-18 16:09:15,801:INFO:Defining folds
2023-07-18 16:09:15,801:INFO:Declaring metric variables
2023-07-18 16:09:15,866:INFO:Importing untrained model
2023-07-18 16:09:15,927:INFO:Logistic Regression Imported successfully
2023-07-18 16:09:16,026:INFO:Starting cross validation
2023-07-18 16:09:16,191:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 16:10:49,388:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:10:49,422:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:10:50,855:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:10:50,861:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:10:52,735:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:10:52,966:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:10:55,261:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:10:55,401:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:10:59,544:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:11:00,368:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:11:05,775:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 2.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:11:07,229:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 2.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:11:09,917:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 3.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:11:11,227:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 2.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:11:17,227:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 2.43s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:11:17,653:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:11:19,703:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:11:21,081:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 2.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:11:22,422:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:11:23,662:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:11:25,244:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:11:25,478:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 3.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:11:31,976:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 2.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:11:32,261:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:11:34,505:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:11:35,371:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:11:37,288:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.35s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:11:39,575:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 2.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:11:40,193:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:11:42,958:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 2.17s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:11:46,952:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 3.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:11:51,157:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 2.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:11:52,103:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 2.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:11:54,423:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:11:55,205:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 2.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:11:55,613:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:11:58,124:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:11:59,284:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:01,121:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:01,488:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:05,468:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:05,750:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 4.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:12:07,935:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:12:11,232:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:13,049:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.47s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:13,090:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:14,681:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:15,298:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:17,114:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:17,950:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:20,237:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:20,254:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:21,698:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:22,855:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:12:25,355:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:26,664:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:27,819:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:28,190:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:29,345:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:12:29,372:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.17s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:12:31,914:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:33,244:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:33,372:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:34,177:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:35,443:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:37,044:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:37,862:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:12:38,720:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:12:39,580:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:41,784:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:42,016:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:43,269:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:43,668:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:12:45,188:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:45,869:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:12:46,593:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:47,825:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:12:48,788:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:50,327:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:51,838:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:12:52,171:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:53,633:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:53,800:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:55,194:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:56,347:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:56,373:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:12:57,405:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:58,029:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:12:58,802:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:13:01,105:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:13:01,810:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:13:01,845:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:13:03,788:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:13:03,954:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:13:05,410:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:13:06,279:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.45s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:13:06,660:INFO:Calculating mean and std
2023-07-18 16:13:06,720:INFO:Creating metrics dataframe
2023-07-18 16:13:06,900:INFO:Uploading results into container
2023-07-18 16:13:06,903:INFO:Uploading model into container now
2023-07-18 16:13:06,905:INFO:_master_model_container: 1
2023-07-18 16:13:06,905:INFO:_display_container: 2
2023-07-18 16:13:06,906:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-18 16:13:06,917:INFO:create_model() successfully completed......................................
2023-07-18 16:14:31,283:INFO:SubProcess create_model() end ==================================
2023-07-18 16:14:31,283:INFO:Creating metrics dataframe
2023-07-18 16:14:31,349:INFO:Initializing K Neighbors Classifier
2023-07-18 16:14:31,350:INFO:Total runtime is 5.265875252087912 minutes
2023-07-18 16:14:31,380:INFO:SubProcess create_model() called ==================================
2023-07-18 16:14:31,381:INFO:Initializing create_model()
2023-07-18 16:14:31,381:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f8d9949a1d0>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 16:14:31,381:INFO:Checking exceptions
2023-07-18 16:14:31,382:INFO:Importing libraries
2023-07-18 16:14:31,382:INFO:Copying training dataset
2023-07-18 16:14:31,441:INFO:Defining folds
2023-07-18 16:14:31,450:INFO:Declaring metric variables
2023-07-18 16:14:31,483:INFO:Importing untrained model
2023-07-18 16:14:31,525:INFO:K Neighbors Classifier Imported successfully
2023-07-18 16:14:31,582:INFO:Starting cross validation
2023-07-18 16:14:31,627:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 16:14:34,164:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:14:34,870:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:14:35,653:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:14:36,381:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:14:37,379:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:14:39,753:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:14:41,285:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:14:42,769:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:14:43,794:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:14:43,966:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:14:48,682:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:14:48,837:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:14:49,958:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:14:50,661:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:14:51,658:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:14:53,518:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:14:54,794:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:14:56,144:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:14:56,322:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:14:58,783:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:14:59,446:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:15:00,550:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:02,522:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:02,905:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.41s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:15:04,571:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:05,823:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:15:06,577:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:08,057:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:09,843:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:15:12,654:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 2.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:15:12,820:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:14,277:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:15,925:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:15:16,736:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:17,098:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:18,810:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:20,524:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:15:20,992:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:15:22,367:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:15:23,034:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:23,932:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:26,929:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 1.32s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:15:27,179:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 16:15:28,321:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:28,753:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:29,921:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:31,525:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:32,394:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:15:33,809:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.41s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:34,881:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:108: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-18 16:15:36,351:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:36,923:INFO:Calculating mean and std
2023-07-18 16:15:36,957:INFO:Creating metrics dataframe
2023-07-18 16:15:37,126:INFO:Uploading results into container
2023-07-18 16:15:37,133:INFO:Uploading model into container now
2023-07-18 16:15:37,139:INFO:_master_model_container: 2
2023-07-18 16:15:37,140:INFO:_display_container: 2
2023-07-18 16:15:37,141:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-18 16:15:37,141:INFO:create_model() successfully completed......................................
2023-07-18 16:15:37,649:INFO:SubProcess create_model() end ==================================
2023-07-18 16:15:37,650:INFO:Creating metrics dataframe
2023-07-18 16:15:37,723:INFO:Initializing Naive Bayes
2023-07-18 16:15:37,724:INFO:Total runtime is 6.372111479441326 minutes
2023-07-18 16:15:37,747:INFO:SubProcess create_model() called ==================================
2023-07-18 16:15:37,748:INFO:Initializing create_model()
2023-07-18 16:15:37,749:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f8d9949a1d0>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 16:15:37,749:INFO:Checking exceptions
2023-07-18 16:15:37,749:INFO:Importing libraries
2023-07-18 16:15:37,750:INFO:Copying training dataset
2023-07-18 16:15:37,848:INFO:Defining folds
2023-07-18 16:15:37,849:INFO:Declaring metric variables
2023-07-18 16:15:37,878:INFO:Importing untrained model
2023-07-18 16:15:37,904:INFO:Naive Bayes Imported successfully
2023-07-18 16:15:37,965:INFO:Starting cross validation
2023-07-18 16:15:38,019:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 16:15:40,750:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:41,791:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:43,751:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:47,543:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:48,116:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:50,287:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:52,411:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.37s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:54,618:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:15:55,733:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:15:58,355:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.17s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:16:00,679:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:01,467:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.24s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:04,173:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:16:06,192:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:16:07,860:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:11,682:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 3.48s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:12,682:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 1.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:16:15,107:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:238: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-18 16:16:17,243:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:19,007:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:19,411:INFO:Calculating mean and std
2023-07-18 16:16:19,422:INFO:Creating metrics dataframe
2023-07-18 16:16:19,601:INFO:Uploading results into container
2023-07-18 16:16:19,605:INFO:Uploading model into container now
2023-07-18 16:16:19,621:INFO:_master_model_container: 3
2023-07-18 16:16:19,622:INFO:_display_container: 2
2023-07-18 16:16:19,623:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-18 16:16:19,623:INFO:create_model() successfully completed......................................
2023-07-18 16:16:20,147:INFO:SubProcess create_model() end ==================================
2023-07-18 16:16:20,153:INFO:Creating metrics dataframe
2023-07-18 16:16:20,226:INFO:Initializing Decision Tree Classifier
2023-07-18 16:16:20,227:INFO:Total runtime is 7.080497554938 minutes
2023-07-18 16:16:20,273:INFO:SubProcess create_model() called ==================================
2023-07-18 16:16:20,274:INFO:Initializing create_model()
2023-07-18 16:16:20,274:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f8d9949a1d0>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 16:16:20,275:INFO:Checking exceptions
2023-07-18 16:16:20,275:INFO:Importing libraries
2023-07-18 16:16:20,275:INFO:Copying training dataset
2023-07-18 16:16:20,366:INFO:Defining folds
2023-07-18 16:16:20,366:INFO:Declaring metric variables
2023-07-18 16:16:20,412:INFO:Importing untrained model
2023-07-18 16:16:20,469:INFO:Decision Tree Classifier Imported successfully
2023-07-18 16:16:20,522:INFO:Starting cross validation
2023-07-18 16:16:20,571:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 16:16:26,113:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:27,410:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:32,437:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:33,270:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:37,289:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.33s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:41,942:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:42,058:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:46,898:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:50,198:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 3.32s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:54,305:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.48s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:16:54,744:INFO:Calculating mean and std
2023-07-18 16:16:54,797:INFO:Creating metrics dataframe
2023-07-18 16:16:55,093:INFO:Uploading results into container
2023-07-18 16:16:55,100:INFO:Uploading model into container now
2023-07-18 16:16:55,103:INFO:_master_model_container: 4
2023-07-18 16:16:55,104:INFO:_display_container: 2
2023-07-18 16:16:55,105:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-18 16:16:55,107:INFO:create_model() successfully completed......................................
2023-07-18 16:16:55,707:INFO:SubProcess create_model() end ==================================
2023-07-18 16:16:55,712:INFO:Creating metrics dataframe
2023-07-18 16:16:55,784:INFO:Initializing SVM - Linear Kernel
2023-07-18 16:16:55,785:INFO:Total runtime is 7.673123725255332 minutes
2023-07-18 16:16:55,838:INFO:SubProcess create_model() called ==================================
2023-07-18 16:16:55,839:INFO:Initializing create_model()
2023-07-18 16:16:55,840:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f8d9949a1d0>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 16:16:55,840:INFO:Checking exceptions
2023-07-18 16:16:55,840:INFO:Importing libraries
2023-07-18 16:16:55,841:INFO:Copying training dataset
2023-07-18 16:16:55,911:INFO:Defining folds
2023-07-18 16:16:55,911:INFO:Declaring metric variables
2023-07-18 16:16:55,945:INFO:Importing untrained model
2023-07-18 16:16:56,014:INFO:SVM - Linear Kernel Imported successfully
2023-07-18 16:16:56,111:INFO:Starting cross validation
2023-07-18 16:16:56,178:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 16:17:01,350:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:01,476:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:04,135:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:04,172:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:09,681:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:09,712:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:10,569:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:10,588:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:15,974:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.39s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:16,041:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:17,233:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:17,278:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:21,152:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:21,163:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:24,674:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:24,709:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:25,216:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:25,236:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:28,319:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:28,332:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 16:17:28,382:INFO:Calculating mean and std
2023-07-18 16:17:28,409:INFO:Creating metrics dataframe
2023-07-18 16:17:28,647:INFO:Uploading results into container
2023-07-18 16:17:28,650:INFO:Uploading model into container now
2023-07-18 16:17:28,652:INFO:_master_model_container: 5
2023-07-18 16:17:28,655:INFO:_display_container: 2
2023-07-18 16:17:28,657:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-18 16:17:28,657:INFO:create_model() successfully completed......................................
2023-07-18 16:17:29,209:INFO:SubProcess create_model() end ==================================
2023-07-18 16:17:29,216:INFO:Creating metrics dataframe
2023-07-18 16:17:29,306:INFO:Initializing Ridge Classifier
2023-07-18 16:17:29,307:INFO:Total runtime is 8.231823952992759 minutes
2023-07-18 16:17:29,346:INFO:SubProcess create_model() called ==================================
2023-07-18 16:17:29,363:INFO:Initializing create_model()
2023-07-18 16:17:29,363:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f8d9949a1d0>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 16:17:29,364:INFO:Checking exceptions
2023-07-18 16:17:29,364:INFO:Importing libraries
2023-07-18 16:17:29,364:INFO:Copying training dataset
2023-07-18 16:17:29,408:INFO:Defining folds
2023-07-18 16:17:29,415:INFO:Declaring metric variables
2023-07-18 16:17:29,431:INFO:Importing untrained model
2023-07-18 16:17:29,491:INFO:Ridge Classifier Imported successfully
2023-07-18 16:17:29,568:INFO:Starting cross validation
2023-07-18 16:17:29,607:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 16:17:33,180:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:17:36,370:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:36,387:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:17:36,413:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:36,492:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:17:41,870:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:41,889:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:17:42,917:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:42,970:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:17:47,150:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:47,174:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:17:50,594:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:50,647:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:17:53,512:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:53,525:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:17:57,684:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:57,698:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:17:59,809:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:17:59,825:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:18:01,781:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:18:01,796:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 16:18:01,871:INFO:Calculating mean and std
2023-07-18 16:18:01,876:INFO:Creating metrics dataframe
2023-07-18 16:18:02,170:INFO:Uploading results into container
2023-07-18 16:18:02,187:INFO:Uploading model into container now
2023-07-18 16:18:02,195:INFO:_master_model_container: 6
2023-07-18 16:18:02,195:INFO:_display_container: 2
2023-07-18 16:18:02,197:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-18 16:18:02,197:INFO:create_model() successfully completed......................................
2023-07-18 16:18:02,653:INFO:SubProcess create_model() end ==================================
2023-07-18 16:18:02,653:INFO:Creating metrics dataframe
2023-07-18 16:18:02,736:INFO:Initializing Random Forest Classifier
2023-07-18 16:18:02,737:INFO:Total runtime is 8.788991526762645 minutes
2023-07-18 16:18:02,766:INFO:SubProcess create_model() called ==================================
2023-07-18 16:18:02,767:INFO:Initializing create_model()
2023-07-18 16:18:02,768:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f8d9949a1d0>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 16:18:02,771:INFO:Checking exceptions
2023-07-18 16:18:02,772:INFO:Importing libraries
2023-07-18 16:18:02,772:INFO:Copying training dataset
2023-07-18 16:18:02,847:INFO:Defining folds
2023-07-18 16:18:02,848:INFO:Declaring metric variables
2023-07-18 16:18:02,877:INFO:Importing untrained model
2023-07-18 16:18:02,944:INFO:Random Forest Classifier Imported successfully
2023-07-18 16:18:03,027:INFO:Starting cross validation
2023-07-18 16:18:03,072:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 16:18:09,217:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:18:09,316:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.39s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:18:13,930:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:18:14,838:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:18:21,635:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:18:23,207:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:18:25,360:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:18:28,419:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:18:31,778:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:18:36,460:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:18:37,519:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:18:41,094:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:18:44,363:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:18:49,469:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 3.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:18:51,364:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 2.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:18:55,115:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:18:57,895:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:19:00,838:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:19:03,438:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 3.33s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:19:04,196:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:19:05,685:INFO:Calculating mean and std
2023-07-18 16:19:05,708:INFO:Creating metrics dataframe
2023-07-18 16:19:05,885:INFO:Uploading results into container
2023-07-18 16:19:05,888:INFO:Uploading model into container now
2023-07-18 16:19:05,889:INFO:_master_model_container: 7
2023-07-18 16:19:05,890:INFO:_display_container: 2
2023-07-18 16:19:05,891:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-18 16:19:05,891:INFO:create_model() successfully completed......................................
2023-07-18 16:19:06,401:INFO:SubProcess create_model() end ==================================
2023-07-18 16:19:06,402:INFO:Creating metrics dataframe
2023-07-18 16:19:06,482:INFO:Initializing Quadratic Discriminant Analysis
2023-07-18 16:19:06,483:INFO:Total runtime is 9.851425655682883 minutes
2023-07-18 16:19:06,546:INFO:SubProcess create_model() called ==================================
2023-07-18 16:19:06,548:INFO:Initializing create_model()
2023-07-18 16:19:06,561:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f8de371ff10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f8d9949a1d0>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 16:19:06,561:INFO:Checking exceptions
2023-07-18 16:19:06,562:INFO:Importing libraries
2023-07-18 16:19:06,562:INFO:Copying training dataset
2023-07-18 16:19:06,635:INFO:Defining folds
2023-07-18 16:19:06,635:INFO:Declaring metric variables
2023-07-18 16:19:06,677:INFO:Importing untrained model
2023-07-18 16:19:06,758:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-18 16:19:06,795:INFO:Starting cross validation
2023-07-18 16:19:06,815:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 16:19:09,161:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 16:19:09,253:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 16:19:10,009:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 16:19:12,970:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:19:14,134:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:19:16,160:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 16:19:18,355:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 16:19:18,423:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:19:21,291:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:19:21,776:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 16:19:23,529:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 16:19:26,737:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 2.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:19:27,013:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 16:19:32,017:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 16:19:32,441:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 16:39:04,959:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-18 16:39:04,994:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-18 16:39:04,994:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-18 16:39:04,994:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-18 20:45:40,870:INFO:PyCaret ClassificationExperiment
2023-07-18 20:45:40,901:INFO:Logging name: mental_health_tech
2023-07-18 20:45:40,901:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-18 20:45:40,901:INFO:version 3.0.4
2023-07-18 20:45:40,901:INFO:Initializing setup()
2023-07-18 20:45:40,902:INFO:self.USI: 1e2e
2023-07-18 20:45:40,902:INFO:self._variable_keys: {'X_test', 'gpu_param', '_ml_usecase', 'log_plots_param', 'y', 'is_multiclass', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'fix_imbalance', 'exp_id', 'exp_name_log', '_available_plots', 'seed', 'idx', 'USI', 'y_test', 'html_param', 'pipeline', 'memory', 'fold_generator', 'n_jobs_param', 'X_train', 'data', 'logging_param', 'fold_shuffle_param', 'X', 'y_train'}
2023-07-18 20:45:40,902:INFO:Checking environment
2023-07-18 20:45:40,902:INFO:python_version: 3.10.8
2023-07-18 20:45:40,902:INFO:python_build: ('main', 'Oct 12 2022 19:14:26')
2023-07-18 20:45:40,902:INFO:machine: x86_64
2023-07-18 20:45:40,902:INFO:platform: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 20:45:40,918:INFO:Memory: svmem(total=3928088576, available=1162924032, percent=70.4, used=2067652608, free=382468096, active=936591360, inactive=1845977088, buffers=99749888, cached=1378217984, shared=388501504, slab=388894720)
2023-07-18 20:45:40,919:INFO:Physical Core: 2
2023-07-18 20:45:40,933:INFO:Logical Core: 2
2023-07-18 20:45:40,934:INFO:Checking libraries
2023-07-18 20:45:40,934:INFO:System:
2023-07-18 20:45:40,934:INFO:    python: 3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]
2023-07-18 20:45:40,934:INFO:executable: /bin/python3.10
2023-07-18 20:45:40,934:INFO:   machine: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 20:45:40,934:INFO:PyCaret required dependencies:
2023-07-18 20:45:55,517:INFO:                 pip: 20.0.2
2023-07-18 20:45:55,518:INFO:          setuptools: 45.2.0
2023-07-18 20:45:55,518:INFO:             pycaret: 3.0.4
2023-07-18 20:45:55,518:INFO:             IPython: 7.34.0
2023-07-18 20:45:55,518:INFO:          ipywidgets: 7.7.5
2023-07-18 20:45:55,518:INFO:                tqdm: 4.65.0
2023-07-18 20:45:55,518:INFO:               numpy: 1.17.4
2023-07-18 20:45:55,519:INFO:              pandas: 1.5.3
2023-07-18 20:45:55,519:INFO:              jinja2: 3.1.2
2023-07-18 20:45:55,519:INFO:               scipy: 1.8.1
2023-07-18 20:45:55,519:INFO:              joblib: 1.2.0
2023-07-18 20:45:55,519:INFO:             sklearn: 1.2.2
2023-07-18 20:45:55,519:INFO:                pyod: 1.1.0
2023-07-18 20:45:55,519:INFO:            imblearn: 0.11.0
2023-07-18 20:45:55,519:INFO:   category_encoders: 2.6.1
2023-07-18 20:45:55,520:INFO:            lightgbm: 4.0.0
2023-07-18 20:45:55,520:INFO:               numba: 0.57.1
2023-07-18 20:45:55,520:INFO:            requests: 2.22.0
2023-07-18 20:45:55,520:INFO:          matplotlib: 3.1.2
2023-07-18 20:45:55,520:INFO:          scikitplot: 0.3.7
2023-07-18 20:45:55,520:INFO:         yellowbrick: 1.5
2023-07-18 20:45:55,520:INFO:              plotly: 5.14.0
2023-07-18 20:45:55,520:INFO:    plotly-resampler: Not installed
2023-07-18 20:45:55,521:INFO:             kaleido: 0.2.1
2023-07-18 20:45:55,521:INFO:           schemdraw: 0.15
2023-07-18 20:45:55,521:INFO:         statsmodels: 0.13.5
2023-07-18 20:45:55,521:INFO:              sktime: 0.20.1
2023-07-18 20:45:55,521:INFO:               tbats: 1.1.3
2023-07-18 20:45:55,521:INFO:            pmdarima: 2.0.3
2023-07-18 20:45:55,521:INFO:              psutil: 5.9.4
2023-07-18 20:45:55,521:INFO:          markupsafe: 1.1.0
2023-07-18 20:45:55,521:INFO:             pickle5: Not installed
2023-07-18 20:45:55,522:INFO:         cloudpickle: 2.2.1
2023-07-18 20:45:55,522:INFO:         deprecation: 2.1.0
2023-07-18 20:45:55,522:INFO:              xxhash: 3.2.0
2023-07-18 20:45:55,522:INFO:           wurlitzer: 3.0.3
2023-07-18 20:45:55,531:INFO:PyCaret optional dependencies:
2023-07-18 20:46:16,770:INFO:                shap: 0.42.0
2023-07-18 20:46:16,770:INFO:           interpret: 0.4.2
2023-07-18 20:46:16,770:INFO:                umap: 0.5.3
2023-07-18 20:46:16,770:INFO:    pandas_profiling: 4.3.1
2023-07-18 20:46:16,770:INFO:  explainerdashboard: 0.4.2.2
2023-07-18 20:46:16,770:INFO:             autoviz: 0.1.730
2023-07-18 20:46:16,771:INFO:           fairlearn: 0.7.0
2023-07-18 20:46:16,771:INFO:          deepchecks: 0.17.3
2023-07-18 20:46:16,771:INFO:             xgboost: 1.7.6
2023-07-18 20:46:16,771:INFO:            catboost: 1.2
2023-07-18 20:46:16,772:INFO:              kmodes: 0.12.2
2023-07-18 20:46:16,772:INFO:             mlxtend: 0.22.0
2023-07-18 20:46:16,774:INFO:       statsforecast: 1.5.0
2023-07-18 20:46:16,774:INFO:        tune_sklearn: 0.4.6
2023-07-18 20:46:16,775:INFO:                 ray: 2.5.1
2023-07-18 20:46:16,775:INFO:            hyperopt: 0.2.7
2023-07-18 20:46:16,775:INFO:              optuna: 3.2.0
2023-07-18 20:46:16,776:INFO:               skopt: 0.9.0
2023-07-18 20:46:16,776:INFO:              mlflow: 1.30.1
2023-07-18 20:46:16,776:INFO:              gradio: 3.37.0
2023-07-18 20:46:16,777:INFO:             fastapi: 0.100.0
2023-07-18 20:46:16,778:INFO:             uvicorn: 0.23.1
2023-07-18 20:46:16,778:INFO:              m2cgen: 0.10.0
2023-07-18 20:46:16,778:INFO:           evidently: 0.2.8
2023-07-18 20:46:16,778:INFO:               fugue: 0.8.5
2023-07-18 20:46:16,779:INFO:           streamlit: 1.22.0
2023-07-18 20:46:16,779:INFO:             prophet: Not installed
2023-07-18 20:46:16,779:INFO:None
2023-07-18 20:46:16,779:INFO:Set up data.
2023-07-18 20:46:27,724:INFO:PyCaret ClassificationExperiment
2023-07-18 20:46:27,728:INFO:Logging name: mental_health_tech
2023-07-18 20:46:27,734:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-18 20:46:27,735:INFO:version 3.0.4
2023-07-18 20:46:27,735:INFO:Initializing setup()
2023-07-18 20:46:27,736:INFO:self.USI: b02e
2023-07-18 20:46:27,747:INFO:self._variable_keys: {'X_test', 'gpu_param', '_ml_usecase', 'log_plots_param', 'y', 'is_multiclass', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'fix_imbalance', 'exp_id', 'exp_name_log', '_available_plots', 'seed', 'idx', 'USI', 'y_test', 'html_param', 'pipeline', 'memory', 'fold_generator', 'n_jobs_param', 'X_train', 'data', 'logging_param', 'fold_shuffle_param', 'X', 'y_train'}
2023-07-18 20:46:27,755:INFO:Checking environment
2023-07-18 20:46:27,757:INFO:python_version: 3.10.8
2023-07-18 20:46:27,758:INFO:python_build: ('main', 'Oct 12 2022 19:14:26')
2023-07-18 20:46:27,758:INFO:machine: x86_64
2023-07-18 20:46:27,758:INFO:platform: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 20:46:27,763:INFO:Memory: svmem(total=3928088576, available=900435968, percent=77.1, used=2269556736, free=127774720, active=927846400, inactive=2092015616, buffers=102662144, cached=1428094976, shared=449183744, slab=388792320)
2023-07-18 20:46:27,769:INFO:Physical Core: 2
2023-07-18 20:46:27,774:INFO:Logical Core: 2
2023-07-18 20:46:27,777:INFO:Checking libraries
2023-07-18 20:46:27,778:INFO:System:
2023-07-18 20:46:27,778:INFO:    python: 3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]
2023-07-18 20:46:27,779:INFO:executable: /bin/python3.10
2023-07-18 20:46:27,779:INFO:   machine: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 20:46:27,779:INFO:PyCaret required dependencies:
2023-07-18 20:46:27,780:INFO:                 pip: 20.0.2
2023-07-18 20:46:27,783:INFO:          setuptools: 45.2.0
2023-07-18 20:46:27,787:INFO:             pycaret: 3.0.4
2023-07-18 20:46:27,787:INFO:             IPython: 7.34.0
2023-07-18 20:46:27,788:INFO:          ipywidgets: 7.7.5
2023-07-18 20:46:27,790:INFO:                tqdm: 4.65.0
2023-07-18 20:46:27,790:INFO:               numpy: 1.17.4
2023-07-18 20:46:27,790:INFO:              pandas: 1.5.3
2023-07-18 20:46:27,791:INFO:              jinja2: 3.1.2
2023-07-18 20:46:27,791:INFO:               scipy: 1.8.1
2023-07-18 20:46:27,791:INFO:              joblib: 1.2.0
2023-07-18 20:46:27,794:INFO:             sklearn: 1.2.2
2023-07-18 20:46:27,794:INFO:                pyod: 1.1.0
2023-07-18 20:46:27,795:INFO:            imblearn: 0.11.0
2023-07-18 20:46:27,801:INFO:   category_encoders: 2.6.1
2023-07-18 20:46:27,802:INFO:            lightgbm: 4.0.0
2023-07-18 20:46:27,802:INFO:               numba: 0.57.1
2023-07-18 20:46:27,807:INFO:            requests: 2.22.0
2023-07-18 20:46:27,811:INFO:          matplotlib: 3.1.2
2023-07-18 20:46:27,812:INFO:          scikitplot: 0.3.7
2023-07-18 20:46:27,815:INFO:         yellowbrick: 1.5
2023-07-18 20:46:27,816:INFO:              plotly: 5.14.0
2023-07-18 20:46:27,816:INFO:    plotly-resampler: Not installed
2023-07-18 20:46:27,816:INFO:             kaleido: 0.2.1
2023-07-18 20:46:27,817:INFO:           schemdraw: 0.15
2023-07-18 20:46:27,817:INFO:         statsmodels: 0.13.5
2023-07-18 20:46:27,817:INFO:              sktime: 0.20.1
2023-07-18 20:46:27,818:INFO:               tbats: 1.1.3
2023-07-18 20:46:27,818:INFO:            pmdarima: 2.0.3
2023-07-18 20:46:27,820:INFO:              psutil: 5.9.4
2023-07-18 20:46:27,820:INFO:          markupsafe: 1.1.0
2023-07-18 20:46:27,821:INFO:             pickle5: Not installed
2023-07-18 20:46:27,821:INFO:         cloudpickle: 2.2.1
2023-07-18 20:46:27,821:INFO:         deprecation: 2.1.0
2023-07-18 20:46:27,822:INFO:              xxhash: 3.2.0
2023-07-18 20:46:27,822:INFO:           wurlitzer: 3.0.3
2023-07-18 20:46:27,822:INFO:PyCaret optional dependencies:
2023-07-18 20:46:27,825:INFO:                shap: 0.42.0
2023-07-18 20:46:27,828:INFO:           interpret: 0.4.2
2023-07-18 20:46:27,833:INFO:                umap: 0.5.3
2023-07-18 20:46:27,834:INFO:    pandas_profiling: 4.3.1
2023-07-18 20:46:27,834:INFO:  explainerdashboard: 0.4.2.2
2023-07-18 20:46:27,834:INFO:             autoviz: 0.1.730
2023-07-18 20:46:27,839:INFO:           fairlearn: 0.7.0
2023-07-18 20:46:27,843:INFO:          deepchecks: 0.17.3
2023-07-18 20:46:27,844:INFO:             xgboost: 1.7.6
2023-07-18 20:46:27,845:INFO:            catboost: 1.2
2023-07-18 20:46:27,845:INFO:              kmodes: 0.12.2
2023-07-18 20:46:27,845:INFO:             mlxtend: 0.22.0
2023-07-18 20:46:27,847:INFO:       statsforecast: 1.5.0
2023-07-18 20:46:27,848:INFO:        tune_sklearn: 0.4.6
2023-07-18 20:46:27,849:INFO:                 ray: 2.5.1
2023-07-18 20:46:27,849:INFO:            hyperopt: 0.2.7
2023-07-18 20:46:27,849:INFO:              optuna: 3.2.0
2023-07-18 20:46:27,850:INFO:               skopt: 0.9.0
2023-07-18 20:46:27,850:INFO:              mlflow: 1.30.1
2023-07-18 20:46:27,854:INFO:              gradio: 3.37.0
2023-07-18 20:46:27,855:INFO:             fastapi: 0.100.0
2023-07-18 20:46:27,857:INFO:             uvicorn: 0.23.1
2023-07-18 20:46:27,857:INFO:              m2cgen: 0.10.0
2023-07-18 20:46:27,858:INFO:           evidently: 0.2.8
2023-07-18 20:46:27,860:INFO:               fugue: 0.8.5
2023-07-18 20:46:27,861:INFO:           streamlit: 1.22.0
2023-07-18 20:46:27,862:INFO:             prophet: Not installed
2023-07-18 20:46:27,863:INFO:None
2023-07-18 20:46:27,866:INFO:Set up data.
2023-07-18 20:46:28,059:INFO:Set up train/test split.
2023-07-18 20:54:48,154:WARNING:/tmp/ipykernel_55460/3059623751.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
  imputed_df[imputed_df.tratamento_profissional_mental.value_counts() > 1]

2023-07-18 20:55:12,792:WARNING:/tmp/ipykernel_55460/4118811300.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
  imputed_df = imputed_df[imputed_df.tratamento_profissional_mental.value_counts() > 1]

2023-07-18 20:58:22,622:INFO:PyCaret ClassificationExperiment
2023-07-18 20:58:22,634:INFO:Logging name: mental_health_tech
2023-07-18 20:58:22,635:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-18 20:58:22,635:INFO:version 3.0.4
2023-07-18 20:58:22,635:INFO:Initializing setup()
2023-07-18 20:58:22,636:INFO:self.USI: 08d8
2023-07-18 20:58:22,636:INFO:self._variable_keys: {'X_test', 'gpu_param', '_ml_usecase', 'log_plots_param', 'y', 'is_multiclass', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'fix_imbalance', 'exp_id', 'exp_name_log', '_available_plots', 'seed', 'idx', 'USI', 'y_test', 'html_param', 'pipeline', 'memory', 'fold_generator', 'n_jobs_param', 'X_train', 'data', 'logging_param', 'fold_shuffle_param', 'X', 'y_train'}
2023-07-18 20:58:22,639:INFO:Checking environment
2023-07-18 20:58:22,640:INFO:python_version: 3.10.8
2023-07-18 20:58:22,643:INFO:python_build: ('main', 'Oct 12 2022 19:14:26')
2023-07-18 20:58:22,650:INFO:machine: x86_64
2023-07-18 20:58:22,650:INFO:platform: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 20:58:22,651:INFO:Memory: svmem(total=3928088576, available=743649280, percent=81.1, used=2444038144, free=130420736, active=1115467776, inactive=1977679872, buffers=63569920, cached=1290059776, shared=442146816, slab=378314752)
2023-07-18 20:58:22,655:INFO:Physical Core: 2
2023-07-18 20:58:22,656:INFO:Logical Core: 2
2023-07-18 20:58:22,657:INFO:Checking libraries
2023-07-18 20:58:22,657:INFO:System:
2023-07-18 20:58:22,658:INFO:    python: 3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]
2023-07-18 20:58:22,658:INFO:executable: /bin/python3.10
2023-07-18 20:58:22,658:INFO:   machine: Linux-5.15.0-53-generic-x86_64-with-glibc2.31
2023-07-18 20:58:22,659:INFO:PyCaret required dependencies:
2023-07-18 20:58:22,659:INFO:                 pip: 20.0.2
2023-07-18 20:58:22,659:INFO:          setuptools: 45.2.0
2023-07-18 20:58:22,660:INFO:             pycaret: 3.0.4
2023-07-18 20:58:22,660:INFO:             IPython: 7.34.0
2023-07-18 20:58:22,660:INFO:          ipywidgets: 7.7.5
2023-07-18 20:58:22,663:INFO:                tqdm: 4.65.0
2023-07-18 20:58:22,663:INFO:               numpy: 1.17.4
2023-07-18 20:58:22,664:INFO:              pandas: 1.5.3
2023-07-18 20:58:22,664:INFO:              jinja2: 3.1.2
2023-07-18 20:58:22,664:INFO:               scipy: 1.8.1
2023-07-18 20:58:22,665:INFO:              joblib: 1.2.0
2023-07-18 20:58:22,665:INFO:             sklearn: 1.2.2
2023-07-18 20:58:22,665:INFO:                pyod: 1.1.0
2023-07-18 20:58:22,666:INFO:            imblearn: 0.11.0
2023-07-18 20:58:22,666:INFO:   category_encoders: 2.6.1
2023-07-18 20:58:22,667:INFO:            lightgbm: 4.0.0
2023-07-18 20:58:22,671:INFO:               numba: 0.57.1
2023-07-18 20:58:22,671:INFO:            requests: 2.22.0
2023-07-18 20:58:22,673:INFO:          matplotlib: 3.1.2
2023-07-18 20:58:22,674:INFO:          scikitplot: 0.3.7
2023-07-18 20:58:22,674:INFO:         yellowbrick: 1.5
2023-07-18 20:58:22,674:INFO:              plotly: 5.14.0
2023-07-18 20:58:22,675:INFO:    plotly-resampler: Not installed
2023-07-18 20:58:22,675:INFO:             kaleido: 0.2.1
2023-07-18 20:58:22,675:INFO:           schemdraw: 0.15
2023-07-18 20:58:22,675:INFO:         statsmodels: 0.13.5
2023-07-18 20:58:22,678:INFO:              sktime: 0.20.1
2023-07-18 20:58:22,679:INFO:               tbats: 1.1.3
2023-07-18 20:58:22,679:INFO:            pmdarima: 2.0.3
2023-07-18 20:58:22,680:INFO:              psutil: 5.9.4
2023-07-18 20:58:22,680:INFO:          markupsafe: 1.1.0
2023-07-18 20:58:22,680:INFO:             pickle5: Not installed
2023-07-18 20:58:22,681:INFO:         cloudpickle: 2.2.1
2023-07-18 20:58:22,681:INFO:         deprecation: 2.1.0
2023-07-18 20:58:22,681:INFO:              xxhash: 3.2.0
2023-07-18 20:58:22,682:INFO:           wurlitzer: 3.0.3
2023-07-18 20:58:22,682:INFO:PyCaret optional dependencies:
2023-07-18 20:58:22,682:INFO:                shap: 0.42.0
2023-07-18 20:58:22,683:INFO:           interpret: 0.4.2
2023-07-18 20:58:22,685:INFO:                umap: 0.5.3
2023-07-18 20:58:22,686:INFO:    pandas_profiling: 4.3.1
2023-07-18 20:58:22,689:INFO:  explainerdashboard: 0.4.2.2
2023-07-18 20:58:22,691:INFO:             autoviz: 0.1.730
2023-07-18 20:58:22,691:INFO:           fairlearn: 0.7.0
2023-07-18 20:58:22,692:INFO:          deepchecks: 0.17.3
2023-07-18 20:58:22,692:INFO:             xgboost: 1.7.6
2023-07-18 20:58:22,692:INFO:            catboost: 1.2
2023-07-18 20:58:22,693:INFO:              kmodes: 0.12.2
2023-07-18 20:58:22,695:INFO:             mlxtend: 0.22.0
2023-07-18 20:58:22,695:INFO:       statsforecast: 1.5.0
2023-07-18 20:58:22,696:INFO:        tune_sklearn: 0.4.6
2023-07-18 20:58:22,696:INFO:                 ray: 2.5.1
2023-07-18 20:58:22,696:INFO:            hyperopt: 0.2.7
2023-07-18 20:58:22,697:INFO:              optuna: 3.2.0
2023-07-18 20:58:22,697:INFO:               skopt: 0.9.0
2023-07-18 20:58:22,697:INFO:              mlflow: 1.30.1
2023-07-18 20:58:22,698:INFO:              gradio: 3.37.0
2023-07-18 20:58:22,698:INFO:             fastapi: 0.100.0
2023-07-18 20:58:22,701:INFO:             uvicorn: 0.23.1
2023-07-18 20:58:22,703:INFO:              m2cgen: 0.10.0
2023-07-18 20:58:22,707:INFO:           evidently: 0.2.8
2023-07-18 20:58:22,707:INFO:               fugue: 0.8.5
2023-07-18 20:58:22,708:INFO:           streamlit: 1.22.0
2023-07-18 20:58:22,708:INFO:             prophet: Not installed
2023-07-18 20:58:22,708:INFO:None
2023-07-18 20:58:22,711:INFO:Set up data.
2023-07-18 20:58:22,934:INFO:Set up train/test split.
2023-07-18 20:58:23,165:INFO:Set up index.
2023-07-18 20:58:23,166:INFO:Set up folding strategy.
2023-07-18 20:58:23,243:INFO:Assigning column types.
2023-07-18 20:58:23,286:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-18 20:58:23,786:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-18 20:58:23,867:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-18 20:58:24,433:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 20:58:25,442:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 20:58:26,694:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-18 20:58:26,698:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-18 20:58:26,858:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 20:58:26,879:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 20:58:26,886:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-18 20:58:27,172:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-18 20:58:27,324:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 20:58:27,337:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 20:58:27,593:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-18 20:58:27,745:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 20:58:27,758:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 20:58:27,760:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-18 20:58:28,130:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 20:58:28,144:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 20:58:28,527:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 20:58:28,541:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 20:58:28,547:INFO:Preparing preprocessing pipeline...
2023-07-18 20:58:28,598:INFO:Set up label encoding.
2023-07-18 20:58:28,599:INFO:Set up simple imputation.
2023-07-18 20:58:28,795:INFO:Finished creating preprocessing pipeline.
2023-07-18 20:58:28,827:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['autonomo', 'num_func', 'emp_tec',
                                             'papel_tec', 'beneficios_mental',
                                             'conhecimento_opcoes_mental',
                                             'discussao_formal_mental',
                                             'recursos_...
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose='deprecated'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated')))],
         verbose=False)
2023-07-18 20:58:28,827:INFO:Creating final display dataframe.
2023-07-18 20:58:29,129:INFO:Setup _display_container:                     Description                           Value
0                    Session id                              42
1                        Target  tratamento_profissional_mental
2                   Target type                      Multiclass
3                Target mapping          1.0: 0, 2.0: 1, 3.0: 2
4           Original data shape                      (1834, 36)
5        Transformed data shape                      (1834, 36)
6   Transformed train set shape                      (1283, 36)
7    Transformed test set shape                       (551, 36)
8              Numeric features                              35
9                    Preprocess                            True
10              Imputation type                          simple
11           Numeric imputation                            mean
12       Categorical imputation                            mode
13               Fold Generator                 StratifiedKFold
14                  Fold Number                              10
15                     CPU Jobs                              -1
16                      Use GPU                           False
17               Log Experiment                           False
18              Experiment Name              mental_health_tech
19                          USI                            08d8
2023-07-18 20:58:29,716:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 20:58:29,733:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 20:58:30,183:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-18 20:58:30,196:INFO:Soft dependency imported: catboost: 1.2
2023-07-18 20:58:30,199:INFO:setup() successfully completed in 7.92s...............
2023-07-18 20:59:24,895:INFO:Initializing compare_models()
2023-07-18 20:59:24,896:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-18 20:59:24,896:INFO:Checking exceptions
2023-07-18 20:59:24,904:INFO:Preparing display monitor
2023-07-18 20:59:25,073:INFO:Initializing Logistic Regression
2023-07-18 20:59:25,073:INFO:Total runtime is 4.621346791585286e-06 minutes
2023-07-18 20:59:25,090:INFO:SubProcess create_model() called ==================================
2023-07-18 20:59:25,163:INFO:Initializing create_model()
2023-07-18 20:59:25,164:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 20:59:25,164:INFO:Checking exceptions
2023-07-18 20:59:25,164:INFO:Importing libraries
2023-07-18 20:59:25,164:INFO:Copying training dataset
2023-07-18 20:59:25,185:INFO:Defining folds
2023-07-18 20:59:25,187:INFO:Declaring metric variables
2023-07-18 20:59:25,235:INFO:Importing untrained model
2023-07-18 20:59:25,268:INFO:Logistic Regression Imported successfully
2023-07-18 20:59:25,367:INFO:Starting cross validation
2023-07-18 20:59:25,377:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 20:59:26,256:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:00:29,720:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:00:30,295:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:00:30,626:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:30,641:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:00:30,654:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:30,676:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:30,706:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:30,710:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:30,718:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:30,722:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:30,726:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:34,258:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:34,268:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:34,274:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:34,278:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:34,554:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:34,557:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:34,571:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:34,584:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:34,595:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:34,617:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:34,633:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:37,641:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-18 21:00:38,278:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:00:38,570:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:38,573:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:38,577:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:38,580:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:38,584:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:38,599:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:38,604:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:38,937:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:00:39,072:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:39,075:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:39,079:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:39,082:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:39,089:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:39,093:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:39,106:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:43,044:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:43,057:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:43,061:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:43,066:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:43,086:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:43,095:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:43,102:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:44,366:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:00:44,559:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:44,562:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:44,581:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:44,585:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:44,589:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:44,593:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:44,597:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:47,439:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:00:48,030:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:48,036:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:48,046:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:48,059:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:48,063:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:48,071:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:48,082:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:48,255:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:48,258:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:48,282:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:48,299:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:48,327:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:48,330:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:48,338:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:48,401:INFO:Calculating mean and std
2023-07-18 21:00:48,409:INFO:Creating metrics dataframe
2023-07-18 21:00:48,871:INFO:Uploading results into container
2023-07-18 21:00:48,876:INFO:Uploading model into container now
2023-07-18 21:00:48,897:INFO:_master_model_container: 1
2023-07-18 21:00:48,897:INFO:_display_container: 2
2023-07-18 21:00:48,921:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-18 21:00:48,921:INFO:create_model() successfully completed......................................
2023-07-18 21:00:49,767:INFO:SubProcess create_model() end ==================================
2023-07-18 21:00:49,768:INFO:Creating metrics dataframe
2023-07-18 21:00:49,799:INFO:Initializing K Neighbors Classifier
2023-07-18 21:00:49,799:INFO:Total runtime is 1.4121056318283083 minutes
2023-07-18 21:00:49,810:INFO:SubProcess create_model() called ==================================
2023-07-18 21:00:49,816:INFO:Initializing create_model()
2023-07-18 21:00:49,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:00:49,817:INFO:Checking exceptions
2023-07-18 21:00:49,818:INFO:Importing libraries
2023-07-18 21:00:49,818:INFO:Copying training dataset
2023-07-18 21:00:49,835:INFO:Defining folds
2023-07-18 21:00:49,836:INFO:Declaring metric variables
2023-07-18 21:00:49,850:INFO:Importing untrained model
2023-07-18 21:00:49,862:INFO:K Neighbors Classifier Imported successfully
2023-07-18 21:00:49,923:INFO:Starting cross validation
2023-07-18 21:00:49,932:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:00:49,952:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:00:50,746:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:50,766:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:50,771:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:50,774:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:50,831:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:50,840:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:50,846:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:50,863:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,400:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,411:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,418:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:51,422:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,876:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:51,878:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,884:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:51,888:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,891:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:51,896:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,905:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:51,907:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:51,910:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,914:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:51,922:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,926:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:51,936:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:51,948:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:52,592:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:52,595:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:52,600:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:52,604:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:52,608:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:52,612:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:52,619:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:52,794:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:52,804:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:52,820:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:52,824:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:52,829:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:52,836:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:52,843:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:53,012:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:53,015:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:53,019:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:53,022:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:53,026:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:53,029:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:53,033:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:53,641:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:53,644:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:53,648:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:53,651:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:53,655:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:53,659:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:53,663:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:53,670:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:53,688:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:53,699:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:53,703:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:53,707:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:53,709:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:53,713:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:53,845:INFO:Calculating mean and std
2023-07-18 21:00:53,856:INFO:Creating metrics dataframe
2023-07-18 21:00:53,997:INFO:Uploading results into container
2023-07-18 21:00:54,002:INFO:Uploading model into container now
2023-07-18 21:00:54,003:INFO:_master_model_container: 2
2023-07-18 21:00:54,004:INFO:_display_container: 2
2023-07-18 21:00:54,005:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-18 21:00:54,005:INFO:create_model() successfully completed......................................
2023-07-18 21:00:54,286:INFO:SubProcess create_model() end ==================================
2023-07-18 21:00:54,287:INFO:Creating metrics dataframe
2023-07-18 21:00:54,331:INFO:Initializing Naive Bayes
2023-07-18 21:00:54,331:INFO:Total runtime is 1.4876405477523806 minutes
2023-07-18 21:00:54,384:INFO:SubProcess create_model() called ==================================
2023-07-18 21:00:54,386:INFO:Initializing create_model()
2023-07-18 21:00:54,387:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:00:54,387:INFO:Checking exceptions
2023-07-18 21:00:54,388:INFO:Importing libraries
2023-07-18 21:00:54,388:INFO:Copying training dataset
2023-07-18 21:00:54,427:INFO:Defining folds
2023-07-18 21:00:54,434:INFO:Declaring metric variables
2023-07-18 21:00:54,533:INFO:Importing untrained model
2023-07-18 21:00:54,604:INFO:Naive Bayes Imported successfully
2023-07-18 21:00:54,716:INFO:Starting cross validation
2023-07-18 21:00:54,719:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:00:54,739:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:00:55,102:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,108:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,112:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:55,116:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,197:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,212:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,232:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:55,235:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,418:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,424:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,429:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:55,432:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,745:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:55,749:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,753:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:55,756:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,761:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:55,765:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,771:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:55,821:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:55,831:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,839:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:55,843:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,851:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:55,861:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:55,883:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:56,290:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:56,293:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:56,307:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:56,311:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:56,315:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:56,317:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:56,321:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:56,357:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:56,364:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:56,369:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:56,372:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:56,377:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:56,389:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:56,394:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:56,879:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:56,899:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:56,904:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:56,907:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:56,911:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:56,922:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:56,927:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:57,037:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:57,052:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:57,065:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:57,073:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:57,077:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:57,094:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:57,124:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:57,432:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:00:57,435:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:57,438:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:57,441:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:57,445:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:57,448:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:57,452:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:00:57,471:INFO:Calculating mean and std
2023-07-18 21:00:57,491:INFO:Creating metrics dataframe
2023-07-18 21:00:57,752:INFO:Uploading results into container
2023-07-18 21:00:57,754:INFO:Uploading model into container now
2023-07-18 21:00:57,755:INFO:_master_model_container: 3
2023-07-18 21:00:57,755:INFO:_display_container: 2
2023-07-18 21:00:57,756:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-18 21:00:57,756:INFO:create_model() successfully completed......................................
2023-07-18 21:00:58,043:INFO:SubProcess create_model() end ==================================
2023-07-18 21:00:58,044:INFO:Creating metrics dataframe
2023-07-18 21:00:58,099:INFO:Initializing Decision Tree Classifier
2023-07-18 21:00:58,099:INFO:Total runtime is 1.5504402319590254 minutes
2023-07-18 21:00:58,111:INFO:SubProcess create_model() called ==================================
2023-07-18 21:00:58,113:INFO:Initializing create_model()
2023-07-18 21:00:58,114:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:00:58,115:INFO:Checking exceptions
2023-07-18 21:00:58,115:INFO:Importing libraries
2023-07-18 21:00:58,116:INFO:Copying training dataset
2023-07-18 21:00:58,162:INFO:Defining folds
2023-07-18 21:00:58,163:INFO:Declaring metric variables
2023-07-18 21:00:58,200:INFO:Importing untrained model
2023-07-18 21:00:58,265:INFO:Decision Tree Classifier Imported successfully
2023-07-18 21:00:58,406:INFO:Starting cross validation
2023-07-18 21:00:58,409:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:00:58,434:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:00:59,582:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:59,590:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:59,594:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:59,600:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:00:59,604:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:59,794:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:59,814:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:00:59,846:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:00:59,852:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,296:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,305:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,308:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:00,312:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,632:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:00,697:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,705:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:00,719:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,725:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:00,742:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,747:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:00,766:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:00,769:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,780:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:00,784:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,789:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:00,793:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:00,797:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:01,333:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:01,336:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:01,340:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:01,343:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:01,349:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:01,354:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:01,356:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:01,360:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:01,363:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:01,367:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:01,370:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:01,374:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:02,927:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:03,088:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:03,091:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:03,095:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:03,100:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:03,104:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:03,112:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:03,118:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:03,442:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:319: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:01:03,451:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:03,453:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:03,457:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:03,460:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:03,466:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:03,746:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:03,749:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:03,753:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:03,757:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:03,761:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:03,765:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:03,769:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:03,826:INFO:Calculating mean and std
2023-07-18 21:01:03,852:INFO:Creating metrics dataframe
2023-07-18 21:01:04,223:INFO:Uploading results into container
2023-07-18 21:01:04,226:INFO:Uploading model into container now
2023-07-18 21:01:04,233:INFO:_master_model_container: 4
2023-07-18 21:01:04,234:INFO:_display_container: 2
2023-07-18 21:01:04,239:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-18 21:01:04,245:INFO:create_model() successfully completed......................................
2023-07-18 21:01:04,683:INFO:SubProcess create_model() end ==================================
2023-07-18 21:01:04,683:INFO:Creating metrics dataframe
2023-07-18 21:01:04,721:INFO:Initializing SVM - Linear Kernel
2023-07-18 21:01:04,722:INFO:Total runtime is 1.6608149051666263 minutes
2023-07-18 21:01:04,752:INFO:SubProcess create_model() called ==================================
2023-07-18 21:01:04,753:INFO:Initializing create_model()
2023-07-18 21:01:04,753:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:01:04,753:INFO:Checking exceptions
2023-07-18 21:01:04,754:INFO:Importing libraries
2023-07-18 21:01:04,754:INFO:Copying training dataset
2023-07-18 21:01:04,782:INFO:Defining folds
2023-07-18 21:01:04,789:INFO:Declaring metric variables
2023-07-18 21:01:04,806:INFO:Importing untrained model
2023-07-18 21:01:04,858:INFO:SVM - Linear Kernel Imported successfully
2023-07-18 21:01:05,000:INFO:Starting cross validation
2023-07-18 21:01:05,004:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:01:05,014:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:01:06,281:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:06,282:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:06,285:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:06,285:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:06,299:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:06,308:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:06,317:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:06,322:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:06,334:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:06,339:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:07,362:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:07,369:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:07,376:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:07,384:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:07,404:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:07,626:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:07,819:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:07,822:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:07,839:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:07,862:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:07,895:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:07,903:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:07,932:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:08,852:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:08,856:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:08,861:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:08,867:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:08,872:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:08,876:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:08,884:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:09,255:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:09,259:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:09,268:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:09,272:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:09,285:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:09,307:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:09,313:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:09,767:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:09,770:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:09,774:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:09,777:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:09,789:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:09,795:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:09,827:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:09,874:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:09,877:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:09,899:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:09,902:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:09,910:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:10,936:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:10,939:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:10,943:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:10,946:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:10,950:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:10,954:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:10,958:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:11,418:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-18 21:01:11,421:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:11,425:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:11,429:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:11,443:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:11,447:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:11,458:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:11,527:INFO:Calculating mean and std
2023-07-18 21:01:11,532:INFO:Creating metrics dataframe
2023-07-18 21:01:11,809:INFO:Uploading results into container
2023-07-18 21:01:11,814:INFO:Uploading model into container now
2023-07-18 21:01:11,816:INFO:_master_model_container: 5
2023-07-18 21:01:11,817:INFO:_display_container: 2
2023-07-18 21:01:11,818:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-18 21:01:11,818:INFO:create_model() successfully completed......................................
2023-07-18 21:01:12,120:INFO:SubProcess create_model() end ==================================
2023-07-18 21:01:12,121:INFO:Creating metrics dataframe
2023-07-18 21:01:12,166:INFO:Initializing Ridge Classifier
2023-07-18 21:01:12,166:INFO:Total runtime is 1.7848896702130639 minutes
2023-07-18 21:01:12,179:INFO:SubProcess create_model() called ==================================
2023-07-18 21:01:12,180:INFO:Initializing create_model()
2023-07-18 21:01:12,181:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:01:12,181:INFO:Checking exceptions
2023-07-18 21:01:12,181:INFO:Importing libraries
2023-07-18 21:01:12,182:INFO:Copying training dataset
2023-07-18 21:01:12,205:INFO:Defining folds
2023-07-18 21:01:12,206:INFO:Declaring metric variables
2023-07-18 21:01:12,220:INFO:Importing untrained model
2023-07-18 21:01:12,242:INFO:Ridge Classifier Imported successfully
2023-07-18 21:01:12,297:INFO:Starting cross validation
2023-07-18 21:01:12,300:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:01:12,326:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:01:13,037:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:13,042:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,065:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,070:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:13,081:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,207:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:13,210:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,220:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,223:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:13,239:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,649:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:13,652:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,670:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,678:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:13,691:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:13,692:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,695:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,699:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:13,702:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,706:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:13,710:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:13,714:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:14,332:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:14,477:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:14,580:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:14,587:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:14,598:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:14,605:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:14,610:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:15,223:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:231: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-18 21:01:15,287:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:15,292:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:15,303:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:15,310:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:15,319:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:15,323:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:15,328:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:15,969:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:15,972:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:15,978:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:15,983:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:15,988:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:15,992:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:16,005:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:16,640:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:16,646:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:16,650:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:16,653:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:16,658:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:16,661:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:16,674:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:17,207:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:01:17,264:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:17,268:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:17,305:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:17,328:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-18 21:01:17,332:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:17,356:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:17,369:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:17,385:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:17,406:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:17,414:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:17,432:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:17,466:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:17,470:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:17,506:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:17,644:INFO:Calculating mean and std
2023-07-18 21:01:17,648:INFO:Creating metrics dataframe
2023-07-18 21:01:17,950:INFO:Uploading results into container
2023-07-18 21:01:17,953:INFO:Uploading model into container now
2023-07-18 21:01:17,954:INFO:_master_model_container: 6
2023-07-18 21:01:17,956:INFO:_display_container: 2
2023-07-18 21:01:17,959:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-18 21:01:17,959:INFO:create_model() successfully completed......................................
2023-07-18 21:01:18,306:INFO:SubProcess create_model() end ==================================
2023-07-18 21:01:18,306:INFO:Creating metrics dataframe
2023-07-18 21:01:18,346:INFO:Initializing Random Forest Classifier
2023-07-18 21:01:18,347:INFO:Total runtime is 1.8878961404164636 minutes
2023-07-18 21:01:18,358:INFO:SubProcess create_model() called ==================================
2023-07-18 21:01:18,359:INFO:Initializing create_model()
2023-07-18 21:01:18,359:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:01:18,360:INFO:Checking exceptions
2023-07-18 21:01:18,360:INFO:Importing libraries
2023-07-18 21:01:18,360:INFO:Copying training dataset
2023-07-18 21:01:18,376:INFO:Defining folds
2023-07-18 21:01:18,377:INFO:Declaring metric variables
2023-07-18 21:01:18,388:INFO:Importing untrained model
2023-07-18 21:01:18,400:INFO:Random Forest Classifier Imported successfully
2023-07-18 21:01:18,423:INFO:Starting cross validation
2023-07-18 21:01:18,426:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:01:18,434:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:01:21,371:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:22,459:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:24,310:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:01:25,533:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:319: UserWarning: Persisting input arguments took 2.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:01:25,586:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:25,635:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:25,638:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:25,644:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:25,647:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:25,651:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:25,652:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:25,654:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:28,591:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:28,626:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:29,624:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:29,650:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:29,656:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:29,660:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:29,680:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:29,685:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:29,692:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:29,711:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:29,729:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:29,743:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:29,764:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:32,743:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:32,743:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:33,835:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:33,847:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:33,852:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:33,861:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:33,866:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:33,863:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:33,872:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:33,886:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:33,887:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:33,893:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:33,897:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:33,916:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:33,920:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:33,932:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:37,862:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:38,043:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:38,791:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:38,795:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:38,810:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:38,815:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:38,819:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:38,827:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:38,831:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:38,888:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:38,890:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:38,894:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:38,897:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:38,912:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:38,915:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:38,921:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:41,246:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:42,206:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:42,209:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:42,213:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:42,220:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:42,224:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:42,227:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:42,231:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:42,255:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:42,258:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:42,262:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:42,269:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:42,273:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:42,276:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:42,286:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:42,376:INFO:Calculating mean and std
2023-07-18 21:01:42,382:INFO:Creating metrics dataframe
2023-07-18 21:01:42,613:INFO:Uploading results into container
2023-07-18 21:01:42,620:INFO:Uploading model into container now
2023-07-18 21:01:42,622:INFO:_master_model_container: 7
2023-07-18 21:01:42,622:INFO:_display_container: 2
2023-07-18 21:01:42,623:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-18 21:01:42,623:INFO:create_model() successfully completed......................................
2023-07-18 21:01:42,875:INFO:SubProcess create_model() end ==================================
2023-07-18 21:01:42,875:INFO:Creating metrics dataframe
2023-07-18 21:01:42,911:INFO:Initializing Quadratic Discriminant Analysis
2023-07-18 21:01:42,913:INFO:Total runtime is 2.2973118265469874 minutes
2023-07-18 21:01:42,933:INFO:SubProcess create_model() called ==================================
2023-07-18 21:01:42,934:INFO:Initializing create_model()
2023-07-18 21:01:42,934:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:01:42,935:INFO:Checking exceptions
2023-07-18 21:01:42,935:INFO:Importing libraries
2023-07-18 21:01:42,935:INFO:Copying training dataset
2023-07-18 21:01:42,950:INFO:Defining folds
2023-07-18 21:01:42,950:INFO:Declaring metric variables
2023-07-18 21:01:42,966:INFO:Importing untrained model
2023-07-18 21:01:42,998:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-18 21:01:43,063:INFO:Starting cross validation
2023-07-18 21:01:43,066:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:01:43,074:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:01:43,373:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:43,375:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:43,755:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:43,761:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:43,767:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:43,770:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:43,904:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:43,917:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:43,921:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:43,926:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:43,980:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:44,150:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:44,154:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:44,162:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:44,170:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:44,173:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:44,388:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:44,540:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:44,550:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:44,564:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:44,567:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:44,589:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:44,592:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:44,596:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:44,906:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:45,050:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:45,051:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:45,053:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,054:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,057:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,058:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,064:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,066:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,068:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,070:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,072:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,072:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,076:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:45,076:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:45,254:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:45,394:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:45,412:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,416:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,419:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,423:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,426:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,430:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:45,623:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:45,675:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:45,816:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:45,819:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,823:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,827:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,837:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,840:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,845:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:45,925:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:45,928:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,932:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,953:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,964:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:45,975:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:45,992:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:46,046:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-18 21:01:46,792:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:01:46,840:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:46,844:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:46,848:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:46,851:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:46,856:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:46,865:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:46,869:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:47,037:INFO:Calculating mean and std
2023-07-18 21:01:47,063:INFO:Creating metrics dataframe
2023-07-18 21:01:47,251:INFO:Uploading results into container
2023-07-18 21:01:47,253:INFO:Uploading model into container now
2023-07-18 21:01:47,255:INFO:_master_model_container: 8
2023-07-18 21:01:47,257:INFO:_display_container: 2
2023-07-18 21:01:47,258:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-18 21:01:47,259:INFO:create_model() successfully completed......................................
2023-07-18 21:01:47,558:INFO:SubProcess create_model() end ==================================
2023-07-18 21:01:47,559:INFO:Creating metrics dataframe
2023-07-18 21:01:47,627:INFO:Initializing Ada Boost Classifier
2023-07-18 21:01:47,628:INFO:Total runtime is 2.37592389980952 minutes
2023-07-18 21:01:47,663:INFO:SubProcess create_model() called ==================================
2023-07-18 21:01:47,664:INFO:Initializing create_model()
2023-07-18 21:01:47,680:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:01:47,680:INFO:Checking exceptions
2023-07-18 21:01:47,681:INFO:Importing libraries
2023-07-18 21:01:47,681:INFO:Copying training dataset
2023-07-18 21:01:47,757:INFO:Defining folds
2023-07-18 21:01:47,757:INFO:Declaring metric variables
2023-07-18 21:01:47,786:INFO:Importing untrained model
2023-07-18 21:01:47,825:INFO:Ada Boost Classifier Imported successfully
2023-07-18 21:01:47,947:INFO:Starting cross validation
2023-07-18 21:01:47,951:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:01:47,963:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:01:50,503:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:50,526:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:50,531:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:50,534:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:50,805:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:01:51,791:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:51,832:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:51,853:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:51,864:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:52,269:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:52,275:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:52,280:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:52,285:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:54,272:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:54,276:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:54,287:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:54,305:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:54,310:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:54,314:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:54,317:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:54,353:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:54,360:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:54,367:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:54,370:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:54,377:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:56,446:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:56,450:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:56,454:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:56,457:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:56,465:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:56,469:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:56,473:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:56,737:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:56,739:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:56,744:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:56,752:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:56,757:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:56,763:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:56,780:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:01:58,569:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:01:58,580:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:58,590:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:58,598:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:58,617:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:01:58,620:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:01:58,632:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:00,405:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:319: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:02:00,511:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:00,514:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:00,531:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:00,535:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:00,540:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:00,544:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:00,548:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:00,606:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:00,624:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:00,648:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:00,655:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:00,661:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:00,667:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:00,671:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:00,826:INFO:Calculating mean and std
2023-07-18 21:02:00,833:INFO:Creating metrics dataframe
2023-07-18 21:02:01,099:INFO:Uploading results into container
2023-07-18 21:02:01,102:INFO:Uploading model into container now
2023-07-18 21:02:01,104:INFO:_master_model_container: 9
2023-07-18 21:02:01,105:INFO:_display_container: 2
2023-07-18 21:02:01,106:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-18 21:02:01,107:INFO:create_model() successfully completed......................................
2023-07-18 21:02:01,376:INFO:SubProcess create_model() end ==================================
2023-07-18 21:02:01,377:INFO:Creating metrics dataframe
2023-07-18 21:02:01,436:INFO:Initializing Gradient Boosting Classifier
2023-07-18 21:02:01,436:INFO:Total runtime is 2.606059082349142 minutes
2023-07-18 21:02:01,453:INFO:SubProcess create_model() called ==================================
2023-07-18 21:02:01,454:INFO:Initializing create_model()
2023-07-18 21:02:01,454:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:02:01,454:INFO:Checking exceptions
2023-07-18 21:02:01,455:INFO:Importing libraries
2023-07-18 21:02:01,455:INFO:Copying training dataset
2023-07-18 21:02:01,484:INFO:Defining folds
2023-07-18 21:02:01,485:INFO:Declaring metric variables
2023-07-18 21:02:01,501:INFO:Importing untrained model
2023-07-18 21:02:01,521:INFO:Gradient Boosting Classifier Imported successfully
2023-07-18 21:02:01,574:INFO:Starting cross validation
2023-07-18 21:02:01,577:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:02:01,584:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:02:08,665:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:08,728:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:10,048:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:10,058:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:10,066:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:10,077:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:10,232:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:10,252:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:10,258:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:10,260:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:17,108:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:17,180:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:18,921:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:18,926:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:18,932:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:18,935:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:18,950:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:19,104:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:19,121:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:19,125:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:19,128:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:25,440:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.48s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:25,924:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:26,923:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:26,927:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:26,934:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:26,937:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:26,941:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:26,944:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:26,951:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:27,431:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:27,433:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:27,442:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:27,445:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:27,449:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:27,452:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:27,458:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:31,933:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:32,957:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:33,320:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:33,325:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:33,330:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:33,338:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:33,345:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:34,703:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:34,705:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:34,709:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:34,712:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:34,716:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:34,719:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:34,723:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:38,644:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:39,740:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:40,694:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:40,697:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:40,701:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:40,705:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:40,712:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:41,532:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:41,542:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:41,547:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:41,550:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:41,554:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:41,558:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:41,562:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:41,619:INFO:Calculating mean and std
2023-07-18 21:02:41,623:INFO:Creating metrics dataframe
2023-07-18 21:02:42,092:INFO:Uploading results into container
2023-07-18 21:02:42,101:INFO:Uploading model into container now
2023-07-18 21:02:42,104:INFO:_master_model_container: 10
2023-07-18 21:02:42,108:INFO:_display_container: 2
2023-07-18 21:02:42,109:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-18 21:02:42,110:INFO:create_model() successfully completed......................................
2023-07-18 21:02:42,474:INFO:SubProcess create_model() end ==================================
2023-07-18 21:02:42,475:INFO:Creating metrics dataframe
2023-07-18 21:02:42,588:INFO:Initializing Linear Discriminant Analysis
2023-07-18 21:02:42,590:INFO:Total runtime is 3.2919511914253237 minutes
2023-07-18 21:02:42,637:INFO:SubProcess create_model() called ==================================
2023-07-18 21:02:42,638:INFO:Initializing create_model()
2023-07-18 21:02:42,639:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:02:42,639:INFO:Checking exceptions
2023-07-18 21:02:42,639:INFO:Importing libraries
2023-07-18 21:02:42,640:INFO:Copying training dataset
2023-07-18 21:02:42,705:INFO:Defining folds
2023-07-18 21:02:42,706:INFO:Declaring metric variables
2023-07-18 21:02:42,733:INFO:Importing untrained model
2023-07-18 21:02:42,782:INFO:Linear Discriminant Analysis Imported successfully
2023-07-18 21:02:42,884:INFO:Starting cross validation
2023-07-18 21:02:42,899:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:02:42,908:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:02:43,520:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:43,542:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:43,569:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:43,585:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:43,813:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:43,820:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:43,824:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:43,828:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,114:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,122:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,129:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:44,132:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,297:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:44,300:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,304:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:44,307:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,318:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,842:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:44,844:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,848:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:44,851:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,872:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,927:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:44,932:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,963:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:44,978:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:44,993:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:45,001:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:45,016:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:45,437:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:45,443:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:45,447:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:45,450:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:45,457:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:45,710:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:45,717:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:45,723:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:45,726:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:45,732:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:46,048:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:46,054:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:46,062:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:46,065:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:46,081:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:46,084:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:46,105:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:46,309:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:46,312:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:46,316:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:46,323:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:46,327:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:46,333:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:46,338:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:02:46,692:INFO:Calculating mean and std
2023-07-18 21:02:46,697:INFO:Creating metrics dataframe
2023-07-18 21:02:46,906:INFO:Uploading results into container
2023-07-18 21:02:46,908:INFO:Uploading model into container now
2023-07-18 21:02:46,910:INFO:_master_model_container: 11
2023-07-18 21:02:46,910:INFO:_display_container: 2
2023-07-18 21:02:46,911:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-18 21:02:46,911:INFO:create_model() successfully completed......................................
2023-07-18 21:02:47,193:INFO:SubProcess create_model() end ==================================
2023-07-18 21:02:47,193:INFO:Creating metrics dataframe
2023-07-18 21:02:47,253:INFO:Initializing Extra Trees Classifier
2023-07-18 21:02:47,254:INFO:Total runtime is 3.369681199391683 minutes
2023-07-18 21:02:47,268:INFO:SubProcess create_model() called ==================================
2023-07-18 21:02:47,272:INFO:Initializing create_model()
2023-07-18 21:02:47,272:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:02:47,272:INFO:Checking exceptions
2023-07-18 21:02:47,272:INFO:Importing libraries
2023-07-18 21:02:47,272:INFO:Copying training dataset
2023-07-18 21:02:47,294:INFO:Defining folds
2023-07-18 21:02:47,294:INFO:Declaring metric variables
2023-07-18 21:02:47,313:INFO:Importing untrained model
2023-07-18 21:02:47,332:INFO:Extra Trees Classifier Imported successfully
2023-07-18 21:02:47,432:INFO:Starting cross validation
2023-07-18 21:02:47,444:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:02:47,476:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:02:50,606:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:50,978:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:51,534:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:51,554:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:51,562:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:51,568:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:53,402:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:319: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:02:53,478:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:53,488:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:53,492:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:53,496:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:55,166:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:55,183:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:55,198:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:55,201:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:56,121:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:58,210:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:02:59,306:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:02:59,345:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:02:59,361:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:59,416:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:59,447:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:59,496:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:02:59,523:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:02:59,529:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:01,099:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:01,104:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:01,109:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:01,112:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:01,117:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:01,124:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:01,128:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:02,421:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:03:03,645:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:03,650:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:03,672:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:03,675:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:03,679:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:03,682:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:03,686:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:03,805:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:03:05,548:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:05,552:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:05,560:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:05,576:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:05,588:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:05,595:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:05,625:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:08,213:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:03:09,001:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:03:09,164:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:09,201:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:09,217:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:09,240:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:09,275:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:09,285:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:09,311:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:10,335:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:10,339:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:10,343:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:10,349:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:10,358:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:10,362:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:10,367:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:12,134:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:12,141:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:12,145:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:12,148:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:12,152:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:12,155:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:12,158:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:12,217:INFO:Calculating mean and std
2023-07-18 21:03:12,221:INFO:Creating metrics dataframe
2023-07-18 21:03:12,473:INFO:Uploading results into container
2023-07-18 21:03:12,475:INFO:Uploading model into container now
2023-07-18 21:03:12,476:INFO:_master_model_container: 12
2023-07-18 21:03:12,477:INFO:_display_container: 2
2023-07-18 21:03:12,480:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-18 21:03:12,480:INFO:create_model() successfully completed......................................
2023-07-18 21:03:13,147:INFO:SubProcess create_model() end ==================================
2023-07-18 21:03:13,148:INFO:Creating metrics dataframe
2023-07-18 21:03:13,255:INFO:Initializing Extreme Gradient Boosting
2023-07-18 21:03:13,269:INFO:Total runtime is 3.8032621264457704 minutes
2023-07-18 21:03:13,300:INFO:SubProcess create_model() called ==================================
2023-07-18 21:03:13,301:INFO:Initializing create_model()
2023-07-18 21:03:13,324:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:03:13,324:INFO:Checking exceptions
2023-07-18 21:03:13,324:INFO:Importing libraries
2023-07-18 21:03:13,324:INFO:Copying training dataset
2023-07-18 21:03:13,369:INFO:Defining folds
2023-07-18 21:03:13,370:INFO:Declaring metric variables
2023-07-18 21:03:13,410:INFO:Importing untrained model
2023-07-18 21:03:13,425:INFO:Extreme Gradient Boosting Imported successfully
2023-07-18 21:03:13,478:INFO:Starting cross validation
2023-07-18 21:03:13,490:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:03:13,502:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:03:22,158:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:22,166:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:22,171:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:22,176:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:22,478:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:22,486:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:22,491:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:22,494:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:26,476:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:26,480:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:26,484:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:26,487:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:26,496:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:26,504:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:26,508:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:27,077:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:27,094:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:27,098:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:27,102:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:29,365:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:29,368:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:29,373:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:29,376:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:29,381:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:29,385:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:29,400:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:30,228:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:30,240:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:30,247:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:30,250:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:30,254:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:30,256:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:30,260:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:32,442:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:32,452:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:32,457:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:32,461:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:32,465:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:32,470:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:32,476:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:33,356:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:33,360:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:33,364:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:33,370:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:33,375:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:33,379:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:33,382:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:35,512:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:35,515:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:35,519:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:35,525:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:35,530:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:35,534:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:35,543:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:36,313:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:36,316:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:36,320:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:36,323:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:36,327:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:36,329:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:36,333:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:03:36,382:INFO:Calculating mean and std
2023-07-18 21:03:36,399:INFO:Creating metrics dataframe
2023-07-18 21:03:36,627:INFO:Uploading results into container
2023-07-18 21:03:36,629:INFO:Uploading model into container now
2023-07-18 21:03:36,633:INFO:_master_model_container: 13
2023-07-18 21:03:36,634:INFO:_display_container: 2
2023-07-18 21:03:36,636:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-18 21:03:36,637:INFO:create_model() successfully completed......................................
2023-07-18 21:03:36,900:INFO:SubProcess create_model() end ==================================
2023-07-18 21:03:36,900:INFO:Creating metrics dataframe
2023-07-18 21:03:36,941:INFO:Initializing Light Gradient Boosting Machine
2023-07-18 21:03:36,941:INFO:Total runtime is 4.197802253564199 minutes
2023-07-18 21:03:36,951:INFO:SubProcess create_model() called ==================================
2023-07-18 21:03:36,952:INFO:Initializing create_model()
2023-07-18 21:03:36,953:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:03:36,953:INFO:Checking exceptions
2023-07-18 21:03:36,953:INFO:Importing libraries
2023-07-18 21:03:36,954:INFO:Copying training dataset
2023-07-18 21:03:36,970:INFO:Defining folds
2023-07-18 21:03:36,971:INFO:Declaring metric variables
2023-07-18 21:03:36,982:INFO:Importing untrained model
2023-07-18 21:03:36,999:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-18 21:03:37,031:INFO:Starting cross validation
2023-07-18 21:03:37,034:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:03:37,043:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:03:45,650:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:45,671:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:45,687:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:45,690:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:45,925:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:45,952:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:45,956:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:45,974:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:54,569:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:54,582:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:54,596:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:54,603:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:54,860:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:03:54,863:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:54,870:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:54,878:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:54,891:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:03:54,900:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:03:54,905:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:04:03,769:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:04:04,271:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:04:04,290:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:04,301:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:04,306:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:04,313:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:04,341:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:04,352:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:04:07,534:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:04:07,557:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:07,571:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:07,577:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:07,586:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:07,591:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:07,599:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:04:14,481:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:04:14,494:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:14,511:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:14,518:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:14,528:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:14,539:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:14,550:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:04:18,400:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:04:18,955:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:04:18,959:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:18,971:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:18,974:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:18,986:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:19,016:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:19,020:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:04:27,457:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:04:27,468:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:27,484:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:27,491:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:27,499:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:27,507:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:27,511:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:04:28,964:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:04:28,968:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:28,972:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:28,976:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:28,982:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:28,989:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:28,993:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:04:29,054:INFO:Calculating mean and std
2023-07-18 21:04:29,082:INFO:Creating metrics dataframe
2023-07-18 21:04:29,405:INFO:Uploading results into container
2023-07-18 21:04:29,409:INFO:Uploading model into container now
2023-07-18 21:04:29,416:INFO:_master_model_container: 14
2023-07-18 21:04:29,416:INFO:_display_container: 2
2023-07-18 21:04:29,418:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-07-18 21:04:29,419:INFO:create_model() successfully completed......................................
2023-07-18 21:04:29,724:INFO:SubProcess create_model() end ==================================
2023-07-18 21:04:29,725:INFO:Creating metrics dataframe
2023-07-18 21:04:29,787:INFO:Initializing CatBoost Classifier
2023-07-18 21:04:29,787:INFO:Total runtime is 5.078573195139567 minutes
2023-07-18 21:04:29,802:INFO:SubProcess create_model() called ==================================
2023-07-18 21:04:29,803:INFO:Initializing create_model()
2023-07-18 21:04:29,804:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:04:29,804:INFO:Checking exceptions
2023-07-18 21:04:29,804:INFO:Importing libraries
2023-07-18 21:04:29,805:INFO:Copying training dataset
2023-07-18 21:04:29,827:INFO:Defining folds
2023-07-18 21:04:29,827:INFO:Declaring metric variables
2023-07-18 21:04:29,875:INFO:Importing untrained model
2023-07-18 21:04:30,009:INFO:CatBoost Classifier Imported successfully
2023-07-18 21:04:30,160:INFO:Starting cross validation
2023-07-18 21:04:30,185:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:04:30,259:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:04:55,894:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:55,902:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:04:55,905:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:04:55,936:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:05:00,230:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:05:00,241:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:05:00,245:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:05:00,249:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:05:26,720:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:05:27,051:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:05:27,058:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:05:27,063:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:05:27,066:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:05:27,069:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:05:27,073:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:05:27,077:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:05:29,636:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:05:30,481:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-18 21:05:30,629:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:05:30,646:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:05:30,652:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:05:30,655:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:00,688:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:06:00,949:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:06:00,952:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:00,957:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:06:00,960:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:00,964:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:06:00,968:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:00,973:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:06:01,425:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:06:01,428:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:01,434:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:06:01,438:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:01,442:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:06:01,446:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:01,452:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:06:31,247:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:06:31,339:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-18 21:06:31,651:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:06:31,656:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:31,663:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:06:31,666:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:31,676:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:06:31,681:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:31,689:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:06:31,849:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:06:31,865:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:31,891:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:06:31,894:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:31,927:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:06:31,936:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:06:31,940:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:00,038:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:07:00,042:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:00,047:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:00,052:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:00,057:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:00,061:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:00,065:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:00,521:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:07:00,534:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:00,541:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:00,545:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:00,549:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:00,552:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:00,556:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:00,619:INFO:Calculating mean and std
2023-07-18 21:07:00,641:INFO:Creating metrics dataframe
2023-07-18 21:07:01,000:INFO:Uploading results into container
2023-07-18 21:07:01,003:INFO:Uploading model into container now
2023-07-18 21:07:01,005:INFO:_master_model_container: 15
2023-07-18 21:07:01,013:INFO:_display_container: 2
2023-07-18 21:07:01,015:INFO:<catboost.core.CatBoostClassifier object at 0x7fe33651d2a0>
2023-07-18 21:07:01,024:INFO:create_model() successfully completed......................................
2023-07-18 21:07:01,386:INFO:SubProcess create_model() end ==================================
2023-07-18 21:07:01,387:INFO:Creating metrics dataframe
2023-07-18 21:07:01,451:INFO:Initializing Dummy Classifier
2023-07-18 21:07:01,451:INFO:Total runtime is 7.6063081463178 minutes
2023-07-18 21:07:01,476:INFO:SubProcess create_model() called ==================================
2023-07-18 21:07:01,477:INFO:Initializing create_model()
2023-07-18 21:07:01,477:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fe345472530>, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:07:01,478:INFO:Checking exceptions
2023-07-18 21:07:01,478:INFO:Importing libraries
2023-07-18 21:07:01,478:INFO:Copying training dataset
2023-07-18 21:07:01,504:INFO:Defining folds
2023-07-18 21:07:01,505:INFO:Declaring metric variables
2023-07-18 21:07:01,520:INFO:Importing untrained model
2023-07-18 21:07:01,530:INFO:Dummy Classifier Imported successfully
2023-07-18 21:07:01,560:INFO:Starting cross validation
2023-07-18 21:07:01,567:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-18 21:07:01,579:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:07:02,020:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:02,026:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:02,030:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:02,033:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:02,148:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:02,154:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:02,161:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:02,165:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:02,612:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:02,650:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:02,664:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:02,672:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:03,242:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:07:03,245:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:03,250:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:03,258:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:03,262:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:03,278:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:03,282:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:03,455:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:07:03,458:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:03,465:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:03,469:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:03,491:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:03,495:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:03,499:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:04,289:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:07:04,337:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:04,373:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:04,377:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:04,389:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:04,393:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:04,397:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:04,424:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:07:04,426:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:04,447:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:04,458:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:04,462:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:04,484:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:04,496:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:05,184:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:07:05,188:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:05,192:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:05,195:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:05,199:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:05,203:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:05,207:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:05,463:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:07:05,475:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:05,487:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:05,511:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:05,519:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:05,522:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:05,551:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:05,879:WARNING:/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 130, in _score
    return super()._score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 566, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 683, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2023-07-18 21:07:05,894:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:05,899:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:05,911:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:05,917:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-18 21:07:05,921:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 3.0) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-18 21:07:05,925:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))

2023-07-18 21:07:06,222:INFO:Calculating mean and std
2023-07-18 21:07:06,239:INFO:Creating metrics dataframe
2023-07-18 21:07:06,471:INFO:Uploading results into container
2023-07-18 21:07:06,496:INFO:Uploading model into container now
2023-07-18 21:07:06,497:INFO:_master_model_container: 16
2023-07-18 21:07:06,497:INFO:_display_container: 2
2023-07-18 21:07:06,497:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-18 21:07:06,497:INFO:create_model() successfully completed......................................
2023-07-18 21:07:06,851:INFO:SubProcess create_model() end ==================================
2023-07-18 21:07:06,855:INFO:Creating metrics dataframe
2023-07-18 21:07:07,057:INFO:Initializing create_model()
2023-07-18 21:07:07,057:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-18 21:07:07,058:INFO:Checking exceptions
2023-07-18 21:07:07,065:INFO:Importing libraries
2023-07-18 21:07:07,066:INFO:Copying training dataset
2023-07-18 21:07:07,089:INFO:Defining folds
2023-07-18 21:07:07,090:INFO:Declaring metric variables
2023-07-18 21:07:07,090:INFO:Importing untrained model
2023-07-18 21:07:07,091:INFO:Declaring custom model
2023-07-18 21:07:07,093:INFO:Random Forest Classifier Imported successfully
2023-07-18 21:07:07,096:INFO:Cross validation set to False
2023-07-18 21:07:07,096:INFO:Fitting Model
2023-07-18 21:07:09,187:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-18 21:07:09,188:INFO:create_model() successfully completed......................................
2023-07-18 21:07:09,776:INFO:_master_model_container: 16
2023-07-18 21:07:09,777:INFO:_display_container: 2
2023-07-18 21:07:09,778:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-18 21:07:09,778:INFO:compare_models() successfully completed......................................
2023-07-18 21:09:19,327:INFO:Initializing evaluate_model()
2023-07-18 21:09:19,491:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-07-18 21:09:20,376:INFO:Initializing plot_model()
2023-07-18 21:09:20,383:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, system=True)
2023-07-18 21:09:20,383:INFO:Checking exceptions
2023-07-18 21:09:20,781:INFO:Preloading libraries
2023-07-18 21:09:21,055:INFO:Copying training dataset
2023-07-18 21:09:21,056:INFO:Plot type: pipeline
2023-07-18 21:09:24,635:INFO:Visual Rendered Successfully
2023-07-18 21:09:24,914:INFO:plot_model() successfully completed......................................
2023-07-18 21:09:44,975:INFO:Initializing plot_model()
2023-07-18 21:09:44,976:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, system=True)
2023-07-18 21:09:44,976:INFO:Checking exceptions
2023-07-18 21:09:45,056:INFO:Preloading libraries
2023-07-18 21:09:45,074:INFO:Copying training dataset
2023-07-18 21:09:45,074:INFO:Plot type: error
2023-07-18 21:09:45,256:INFO:Fitting Model
2023-07-18 21:09:45,256:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2023-07-18 21:09:45,257:INFO:Scoring test/hold-out set
2023-07-18 21:09:46,061:INFO:Visual Rendered Successfully
2023-07-18 21:09:46,540:INFO:plot_model() successfully completed......................................
2023-07-18 21:09:56,649:INFO:Initializing plot_model()
2023-07-18 21:09:56,652:INFO:plot_model(plot=vc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, system=True)
2023-07-18 21:09:56,652:INFO:Checking exceptions
2023-07-18 21:09:56,694:INFO:Preloading libraries
2023-07-18 21:09:56,713:INFO:Copying training dataset
2023-07-18 21:09:56,713:INFO:Plot type: vc
2023-07-18 21:09:56,715:INFO:Determining param_name
2023-07-18 21:09:56,715:INFO:param_name: max_depth
2023-07-18 21:09:56,951:INFO:Fitting Model
2023-07-18 21:09:56,997:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2023-07-18 21:10:39,819:INFO:Visual Rendered Successfully
2023-07-18 21:10:40,124:INFO:plot_model() successfully completed......................................
2023-07-18 21:10:40,286:INFO:Initializing evaluate_model()
2023-07-18 21:10:40,286:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-07-18 21:10:40,394:INFO:Initializing plot_model()
2023-07-18 21:10:40,394:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, system=True)
2023-07-18 21:10:40,395:INFO:Checking exceptions
2023-07-18 21:10:40,641:INFO:Preloading libraries
2023-07-18 21:10:40,702:INFO:Copying training dataset
2023-07-18 21:10:40,703:INFO:Plot type: pipeline
2023-07-18 21:10:41,959:INFO:Visual Rendered Successfully
2023-07-18 21:10:42,761:INFO:plot_model() successfully completed......................................
2023-07-18 21:10:42,854:INFO:Initializing plot_model()
2023-07-18 21:10:42,860:INFO:plot_model(plot=feature, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, system=True)
2023-07-18 21:10:42,865:INFO:Checking exceptions
2023-07-18 21:10:43,111:INFO:Preloading libraries
2023-07-18 21:10:43,332:INFO:Copying training dataset
2023-07-18 21:10:43,333:INFO:Plot type: feature
2023-07-18 21:10:43,361:WARNING:No coef_ found. Trying feature_importances_
2023-07-18 21:10:45,437:INFO:Visual Rendered Successfully
2023-07-18 21:10:45,913:INFO:plot_model() successfully completed......................................
2023-07-18 21:10:48,257:INFO:Initializing plot_model()
2023-07-18 21:10:48,266:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fe33682f1f0>, system=True)
2023-07-18 21:10:48,267:INFO:Checking exceptions
2023-07-18 21:10:48,399:INFO:Preloading libraries
2023-07-18 21:10:48,634:INFO:Copying training dataset
2023-07-18 21:10:48,637:INFO:Plot type: confusion_matrix
2023-07-18 21:10:50,085:INFO:Fitting Model
2023-07-18 21:10:50,099:WARNING:/home/jesse/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2023-07-18 21:10:50,101:INFO:Scoring test/hold-out set
2023-07-18 21:10:51,406:INFO:Visual Rendered Successfully
2023-07-18 21:10:52,661:INFO:plot_model() successfully completed......................................
